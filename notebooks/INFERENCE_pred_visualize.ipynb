{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edb2117-21ba-420d-8971-ff5b28cd834f",
   "metadata": {},
   "source": [
    "# Inference on validation data\n",
    "\n",
    "This notebook provides how to make prediction and visualize movement prediction to gif files.\n",
    "\n",
    "Also you can stack prediction horizontally for comparing difference between prediction and real movements \n",
    "\n",
    "Default parameters: \n",
    "\n",
    "- FPS = 200\n",
    "- NEW_FPS = 25\n",
    "- WINDOW_SIZE = 512\n",
    "- STRIDE = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97dd99a-9c28-44d7-938d-d4d007a0a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb, sys, os  \n",
    "import numpy as np\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "from utils import data_utils, losses\n",
    "from utils.hand_visualize import Hand, save_animation_mp4, visualize_and_save_anim, merge_two_videos, visualize_and_save_anim_gifs #, merge_two_videos_vertically\n",
    "from utils.inference_utils import make_inference, calculcate_latency, get_angle_degree\n",
    "from utils.quats_and_angles import get_quats, get_angles\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "from models import HVATNet_v2, HVATNet_v3, HVATNet_v3_FineTune, HVATNet_v3_FineTune_N\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fcd49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c927de0b-dbfb-404f-a946-96e394ca8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "def upload_weights_pl(model, path, pt):\n",
    "    class Lit_Wrapper(pl.LightningModule):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "        def forward(self, x):\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "    \n",
    "    if pt == 'ckpt':\n",
    "        ckpt = torch.load(path, map_location=torch.device('cpu'))\n",
    "        model_pl = Lit_Wrapper(model)\n",
    "        model_pl.load_state_dict(ckpt['state_dict'])\n",
    "    else:\n",
    "        try:\n",
    "            ckpt = torch.load(path, map_location=torch.device('cpu'))\n",
    "            model_pl = Lit_Wrapper(model)\n",
    "            model_pl.load_state_dict(ckpt)\n",
    "        except:\n",
    "            ckpt = torch.load(path, map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(ckpt)\n",
    "            return model\n",
    "    \n",
    "    return model_pl.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "409480f7-9141-460a-b626-7619cb69b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    WANDB_NOTES = 'HVATNet v3 FT train on all new data + no augs'\n",
    "\n",
    "    datasets = ['../../data/general_set']\n",
    "    # datasets = ['../../data/processed/dataset_v2_blocks']\n",
    "    \n",
    "    # hand_type = ['right']\n",
    "    # human_type = ['health']\n",
    "    hand_type = ['right']\n",
    "    human_type = ['amputant']\n",
    "\n",
    "    test_dataset_list = ['5']\n",
    "\n",
    "    use_preproc_data = True # use preproc data (faster preparation  \n",
    "    use_angles = True # use angels as target.\n",
    "    \n",
    "    original_fps = 250  # TODO describtion\n",
    "    delay_ms = 0  # Shift vr vs EMG parameter. Do not work with preproc data. Fix it!!\n",
    "    start_crop_ms = 0  # bad values in the beginning of recordign in ms to delete.\n",
    "    window_size = 256\n",
    "    down_sample_target=8 # None\n",
    "\n",
    "    max_epochs = 3000\n",
    "    samples_per_epoch = 5000*256\n",
    "    train_bs = 2048\n",
    "    val_bs = 2048\n",
    "    device = [2]\n",
    "    optimizer_params = dict(lr=1e-4,\n",
    "                            wd=0)\n",
    "config = TrainConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05ddd76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = Path('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train')\n",
    "files = list(rootdir.glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3616e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0af0bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/1_1'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/1_2'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/1_3'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/1_4'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/2_1'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/2_2'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/2_3'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/2_4'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/3_1'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/3_2'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/3_3'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/3_4'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/4_1'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/4_2'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/4_3'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/4_4'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/4_5'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/5_1'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/5_3'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/5_4'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train/m3_5')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86974c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = Path('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Validation')\n",
    "files = list(rootdir.glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f36ca4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_paths = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "569ee5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Validation/1_1')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008466f-6d56-4f30-a42a-0df8c26737db",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beee5010-22dd-4bcf-88c1-06052279a86d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of moves: 8 | Dataset: GeneralTraining\n"
     ]
    }
   ],
   "source": [
    "# Init train and val dataset and dataloaders\n",
    "val_datasets = []\n",
    "for val_folder in val_paths:\n",
    "    val_dataset = data_utils.create_dataset(data_folder=val_folder,\n",
    "                                            original_fps=config.original_fps,\n",
    "                                            delay_ms=config.delay_ms,\n",
    "                                            start_crop_ms=config.start_crop_ms,\n",
    "                                            window_size=config.window_size,\n",
    "\n",
    "                                            down_sample_target = config.down_sample_target, \n",
    "                                            use_preproc_data=config.use_preproc_data, \n",
    "                                            use_angles=config.use_angles, \n",
    "\n",
    "                                            random_sampling = False,\n",
    "                                            samples_per_epoch = None, \n",
    "                                            transform = None)\n",
    "    \n",
    "    val_datasets.append(val_dataset)\n",
    "\n",
    "val_dataset = torch.utils.data.ConcatDataset(val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee498b63-17c5-4f9b-a0ba-8e8e3ac67f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.ConcatDataset(val_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1a573-65ec-4d5d-a441-94820a686e9e",
   "metadata": {},
   "source": [
    "## Init model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86f5f0b4-f120-44d3-bc2a-e8cc6f7f113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  4210788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/vlvdi/Desktop/EMG/MainScripts/weights/'\n",
    "\n",
    "CKPT_PATH = root + 'latest_simple_nast.pt'\n",
    "#CKPT_PATH = artifact_dir / 'epoch=23_val_angle_degree=8.682.ckpt'\n",
    "\n",
    "params =dict(n_electrodes=8, n_channels_out=20,\n",
    "                        n_res_blocks=3, n_blocks_per_layer=3,\n",
    "                        n_filters=128, kernel_size=3,\n",
    "                        strides=(2, 2, 2),\n",
    "                        dilation=2, use_angles = False)\n",
    "\n",
    "model = HVATNet_v3_FineTune_N.HVATNetv3(**params)\n",
    "model.use_angles = True\n",
    "\n",
    "model = upload_weights_pl(model, CKPT_PATH, CKPT_PATH[-4:])\n",
    "\n",
    "#model.load_state_dict(torch.load(CKPT_PATH, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "print()\n",
    "\n",
    "# TO SAVE MODEL:\n",
    "#torch.save(model.state_dict(), 'C:/Users/vlvdi/Desktop/EMG/model_nast_simple.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "918e84d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9349,  0.0073,  0.2019,  0.1432,  0.0932,  0.1879,  0.0873,  0.0327],\n",
       "        [ 0.0045,  0.9120,  0.0723,  0.1233,  0.1115,  0.1561,  0.1423,  0.1124],\n",
       "        [ 0.0797, -0.0847,  0.7601, -0.0369,  0.1082,  0.1474,  0.1955,  0.1603],\n",
       "        [ 0.0771,  0.0754,  0.0791,  0.8439,  0.0322,  0.1315,  0.1551,  0.1556],\n",
       "        [ 0.0760,  0.1414,  0.2265,  0.0222,  0.8946, -0.0027,  0.0666,  0.1134],\n",
       "        [ 0.0690,  0.1321,  0.2582,  0.1089, -0.0251,  0.8681,  0.0077,  0.0501],\n",
       "        [ 0.0108,  0.0942,  0.2647,  0.1532,  0.0895,  0.0313,  0.8688,  0.0154],\n",
       "        [-0.0199,  0.0586,  0.2243,  0.1930,  0.1227,  0.1052,  0.0147,  0.8787]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['tune_module.spatial_weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b72710-0f49-4c00-92a5-2db7a4ac0c9f",
   "metadata": {},
   "source": [
    "## Apply inference for each moves.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "491cb9d8-cacb-455e-8fb7-df938957ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.ConcatDataset(val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "845996ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "34031782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.use_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6e217dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "56314622-435b-432e-940d-b7e436b5787f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of targets (2752, 20) || Size of preds (2752, 20) \n",
      "Window size 256 || Stride 256\n",
      "Angle degree:  3.702 0.0\n",
      "Cosine similarity:  0.893 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "FPS = 25\n",
    "NEW_FPS = 25\n",
    "DRAW_EVERY = 200 // NEW_FPS\n",
    "\n",
    "WINDOW_SIZE = 256\n",
    "STRIDE = 256\n",
    "\n",
    "corr_list = []\n",
    "angle_degree_list = []\n",
    "preds_per_person = []\n",
    "targets_per_person = []\n",
    "abs_diff_per_person = []\n",
    "angle_diff_per_person = []\n",
    "\n",
    "for val_dataset_sample in val_dataset.datasets:\n",
    "    \n",
    "    all_move_targets = []\n",
    "    all_move_preds = []\n",
    "    all_myo = []\n",
    "    \n",
    "    for move_data in tqdm(val_dataset_sample.exps_data): \n",
    "        data_myo, data_vr = move_data['data_myo'], move_data['data_angles']\n",
    "\n",
    "        preds, targets = make_inference(data_myo, data_vr, model,\n",
    "                                        window_size=WINDOW_SIZE, \n",
    "                                        stride=STRIDE, \n",
    "                                        device='cpu', \n",
    "                                        return_angles=True)\n",
    "        preds = get_angles(preds)\n",
    "        if config.down_sample_target is not None:\n",
    "            targets = targets[::config.down_sample_target]\n",
    "\n",
    "        all_move_targets.append(targets)\n",
    "        all_move_preds.append(preds)\n",
    "        all_myo.append(data_myo)\n",
    "\n",
    "    targets = np.concatenate(all_move_targets, axis=0)\n",
    "    preds = np.concatenate(all_move_preds, axis=0)\n",
    "    all_myo = np.concatenate(all_myo, axis=0)\n",
    "\n",
    "    preds_per_person.append(all_move_preds)\n",
    "    targets_per_person.append(all_move_targets)\n",
    "    diff = np.abs(targets - preds)\n",
    "    abs_diff_per_person.append(diff)\n",
    "    mean_diff_angle_per_joint = np.rad2deg(np.mean(diff, axis = 1))\n",
    "    angle_diff_per_person.append(mean_diff_angle_per_joint)\n",
    "    \n",
    "    # our metrics:\n",
    "    dif = np.mean(diff)\n",
    "    angle_degree = np.round(np.rad2deg(dif), 3)\n",
    "\n",
    "    corr_coef = torch.mean(F.cosine_similarity(torch.from_numpy(targets), torch.from_numpy(preds), dim=0, eps=1e-8))\n",
    "    corr_coef = np.round(corr_coef.item(), 3)\n",
    "    \n",
    "    corr_list.append(corr_coef)\n",
    "    angle_degree_list.append(angle_degree)\n",
    "    \n",
    "\n",
    "# angle_degree = np.round(F.preds, targets), 3)\n",
    "\n",
    "print(f\"Size of targets {targets.shape} || Size of preds {preds.shape} \") \n",
    "print(f\"Window size {WINDOW_SIZE} || Stride {STRIDE}\")\n",
    "print('Angle degree: ', np.mean(angle_degree_list), np.std(angle_degree_list))\n",
    "print('Cosine similarity: ', np.mean(corr_list), np.std(corr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9a736e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_quats = get_quats(preds)\n",
    "tar_quats = get_quats(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ec5ae983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video test_predict_offline completed\n"
     ]
    }
   ],
   "source": [
    "NEW_FPS = 25\n",
    "DRAW_EVERY = 4\n",
    "visualize_and_save_anim_gifs(data=pred_quats[::DRAW_EVERY],\n",
    "                        path=Path('C:/Users/vlvdi/Desktop/EMG/test_predict_offline.gif'), \n",
    "                        fps=NEW_FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_and_save_anim_gifs(data=tar_quats[::DRAW_EVERY],\n",
    "                        path=Path('C:/Users/vlvdi/Desktop/EMG/test_predict_offline.gif'), \n",
    "                        fps=NEW_FPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
