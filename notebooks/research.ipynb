{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f687a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Установка seed для стандартного генератора случайных чисел Python\n",
    "random.seed(42)\n",
    "\n",
    "# Установка seed для NumPy (если используете его)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Установка seed для PyTorch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Если используете CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)  # Если у вас несколько GPU\n",
    "\n",
    "# Для обеспечения полной повторяемости (этот шаг замедляет выполнение на GPU)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354cd044",
   "metadata": {},
   "source": [
    "### Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba8e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "subj_path = \"..\\\\data\\\\Fedor\\\\Raw\\\\preproc_angles\\\\1\\\\\"\n",
    "fs = 500\n",
    "\n",
    "\n",
    "def corrcoef(x, y):\n",
    "    if np.std(x) == 0 or np.std(y) == 0:\n",
    "        return 0.0\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "\n",
    "def train_test_split(data, N_parts, num_of_part):\n",
    "    N_samples = len(data)\n",
    "\n",
    "    l_idx = int((N_samples * num_of_part) / N_parts)\n",
    "    h_idx = int((N_samples * (num_of_part + 1)) / N_parts)\n",
    "\n",
    "    data_train = np.concatenate([data[:l_idx, :], data[h_idx:,]], axis=0)\n",
    "    data_test = data[l_idx:h_idx, :]\n",
    "\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "N_files = 4\n",
    "data_list_train = list()\n",
    "data_list_test = list()\n",
    "label_list_train = list()\n",
    "label_list_test = list()\n",
    "\n",
    "N_parts = 10\n",
    "num_of_part = 9\n",
    "\n",
    "for i in range(N_files):\n",
    "\n",
    "    arr = np.load(subj_path + \"000\" + str(i) + \".npz\")\n",
    "\n",
    "    std_coef = arr[\"std_coef\"]\n",
    "    data = arr[\"data_myo\"]\n",
    "    label = arr[\"data_angles\"]\n",
    "\n",
    "    data_train, data_test = train_test_split(data, N_parts, num_of_part)\n",
    "    label_train, label_test = train_test_split(label, N_parts, num_of_part)\n",
    "\n",
    "    data_list_train.append(data_train)\n",
    "    data_list_test.append(data_test)\n",
    "\n",
    "    label_list_train.append(label_train)\n",
    "    label_list_test.append(label_test)\n",
    "\n",
    "\n",
    "data_train = np.concatenate(data_list_train, axis=0)\n",
    "data_test = np.concatenate(data_list_test, axis=0)\n",
    "\n",
    "\n",
    "label_train = np.concatenate(label_list_train, axis=0)\n",
    "label_test = np.concatenate(label_list_test, axis=0)\n",
    "\n",
    "\n",
    "def slicer(data, label, fs, windowlen=500, timestep=100):\n",
    "    data_len = len(data)\n",
    "    timestep_samples = int((timestep * fs) / 1000)\n",
    "    windowlen_samples = int((windowlen * fs) / 1000)\n",
    "    start_idc = np.arange(0, data_len - windowlen_samples, timestep_samples)[:, None]\n",
    "    window_idc = np.arange(0, windowlen_samples)[None, :]\n",
    "    slice_idc = start_idc + window_idc\n",
    "    slice_data = data[slice_idc].transpose(0, 2, 1)\n",
    "    slice_label = label[start_idc[:, 0] + windowlen_samples]\n",
    "    return slice_data, slice_label\n",
    "\n",
    "\n",
    "X_train, y_train = slicer(data_train, label_train, fs, windowlen=256, timestep=200)\n",
    "X_test, y_test = slicer(data_test, label_test, fs, windowlen=256, timestep=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb721bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1599, 6, 128), (177, 6, 128), (1599, 20), (177, 20))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604e3ee",
   "metadata": {},
   "source": [
    "## Riemann Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cadeb4",
   "metadata": {},
   "source": [
    "### Covariances Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa69da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 6, 1000)\n",
    "\n",
    "# Применим ковариацию с OAS\n",
    "cov_estimator = Covariances(estimator=\"oas\")\n",
    "cov_matrices = cov_estimator.fit_transform(X)\n",
    "\n",
    "print(cov_matrices.shape)  # (10, 6, 6)\n",
    "print(cov_matrices[0])\n",
    "\n",
    "\n",
    "def oas_covariance(X):\n",
    "    \"\"\"OAS shrinkage covariance estimation for a single (channels x samples) matrix\"\"\"\n",
    "    X = X - X.mean(axis=1, keepdims=True)  # центрируем по каналам\n",
    "    n_channels, n_samples = X.shape\n",
    "\n",
    "    # Стандартная ковариация\n",
    "    sample_cov = (X @ X.T) / (n_samples - 1)\n",
    "\n",
    "    # След и его квадрат\n",
    "    trace_S = np.trace(sample_cov)\n",
    "    trace_S2 = np.sum(sample_cov**2)\n",
    "\n",
    "    # Оценка \"alpha\" для shrinkage\n",
    "    shrinkage = min(\n",
    "        1.0,\n",
    "        ((1 - 2 / n_channels) * trace_S2 + trace_S**2)\n",
    "        / ((n_samples + 1 - 2 / n_channels) * (trace_S2 - (trace_S**2) / n_channels)),\n",
    "    )\n",
    "\n",
    "    print(shrinkage)\n",
    "    # Shrink к матрице I * (trace / n_channels)\n",
    "    mu = trace_S / n_channels\n",
    "    shrunk_cov = (1 - shrinkage) * sample_cov + shrinkage * mu * np.eye(n_channels)\n",
    "    return shrunk_cov\n",
    "\n",
    "\n",
    "def compute_covariances_oas(X):\n",
    "    \"\"\"X: shape (n_trials, n_channels, n_times)\"\"\"\n",
    "    return np.array([oas_covariance(trial) for trial in X])\n",
    "\n",
    "\n",
    "cov1 = Covariances(estimator=\"oas\").fit_transform(X)\n",
    "\n",
    "# Используем свою реализацию\n",
    "cov2 = compute_covariances_oas(X)\n",
    "\n",
    "# Проверим разницу\n",
    "print(\"Equal:\")\n",
    "print(np.allclose(cov1, cov2))\n",
    "\n",
    "print(\"Cov Matrices:\")\n",
    "print(cov1[0], cov2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Симуляция данных: 10 эпох, 6 каналов, 1000 отсчётов времени\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(10, 6, 1000)\n",
    "\n",
    "# Расчёт ковариационных матриц с использованием OAS\n",
    "cov_estimator = Covariances(estimator=\"oas\")\n",
    "cov_matrices = cov_estimator.fit_transform(X)\n",
    "\n",
    "# Выбираем первую ковариационную матрицу для визуализации\n",
    "cov_matrix = cov_matrices[0]\n",
    "\n",
    "# Создаём тепловую карту\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cov_matrix, cmap=\"viridis\", interpolation=\"none\")\n",
    "plt.title(\"Ковариационная матрица (OAS) для первой эпохи\")\n",
    "plt.colorbar(label=\"Значение ковариации\")\n",
    "plt.xlabel(\"Каналы\")\n",
    "plt.ylabel(\"Каналы\")\n",
    "plt.xticks(range(cov_matrix.shape[0]))\n",
    "plt.yticks(range(cov_matrix.shape[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6107be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_estimator = Covariances(estimator=\"oas\").fit(X_train)\n",
    "cov1 = cov_estimator.transform(X_train)\n",
    "cov_matrix = cov1[5]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cov_matrix, cmap=\"viridis\", interpolation=\"none\")\n",
    "plt.title(\"Ковариационная матрица (OAS) для первой эпохи\")\n",
    "plt.colorbar(label=\"Значение ковариации\")\n",
    "plt.xlabel(\"Каналы\")\n",
    "plt.ylabel(\"Каналы\")\n",
    "plt.xticks(range(cov_matrix.shape[0]))\n",
    "plt.yticks(range(cov_matrix.shape[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_estimator = Covariances(estimator=\"oas\").fit(X_train_emg)\n",
    "cov1 = cov_estimator.transform(X_train_emg)\n",
    "cov_matrix = cov1[0]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cov_matrix, cmap=\"viridis\", interpolation=\"none\")\n",
    "plt.title(\"Ковариационная матрица (OAS) для первой эпохи\")\n",
    "plt.colorbar(label=\"Значение ковариации\")\n",
    "plt.xlabel(\"Каналы\")\n",
    "plt.ylabel(\"Каналы\")\n",
    "plt.xticks(range(cov_matrix.shape[0]))\n",
    "plt.yticks(range(cov_matrix.shape[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30815da9",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiemannMLPRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=\"oas\",\n",
    "        metric=\"riemann\",\n",
    "        hidden_layer_sizes=(200,),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-5,\n",
    "        max_iter=200,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.estimator = estimator\n",
    "        self.metric = metric\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # Внутренние объекты моделей\n",
    "        self.cov_ = None\n",
    "        self.ts_ = None\n",
    "        self.reg_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 1. Ковариации\n",
    "        self.cov_ = Covariances(estimator=self.estimator)\n",
    "        X_cov = self.cov_.fit_transform(X)\n",
    "        print(\"shape of X_cov:\", X_cov.shape)\n",
    "\n",
    "        # 2. Tangent Space\n",
    "        self.ts_ = TangentSpace(metric=self.metric)\n",
    "        X_ts = self.ts_.fit_transform(X_cov)\n",
    "        print(\"shape of X_ts:\", X_ts.shape)\n",
    "\n",
    "        # 3. MLP Regressor\n",
    "        self.reg_ = MLPRegressor(\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver=self.solver,\n",
    "            alpha=self.alpha,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "        self.reg_.fit(X_ts, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Обработка новых данных\n",
    "        X_cov = self.cov_.transform(X)\n",
    "        X_ts = self.ts_.transform(X_cov)\n",
    "        return self.reg_.predict(X_ts)\n",
    "\n",
    "\n",
    "model = RiemannMLPRegressor(\n",
    "    estimator=\"oas\",\n",
    "    metric=\"riemann\",\n",
    "    hidden_layer_sizes=(400, 400),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=1e-5,\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_model_pred = model.predict(X_test)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    Covariances(\"oas\"),\n",
    "    TangentSpace(metric=\"riemann\"),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(400, 400),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=0.00001,\n",
    "        random_state=42,\n",
    "    ),\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pipeline_pred = pipeline.predict(X_test)\n",
    "\n",
    "results = {}\n",
    "\n",
    "correlations = []\n",
    "for i in range(y_test.shape[1]):\n",
    "    try:\n",
    "        corr = corrcoef(y_model_pred[:, i], y_test[:, i])\n",
    "        correlations.append(corr)\n",
    "    except:\n",
    "        correlations.append(0.0)\n",
    "\n",
    "results[\"model\"] = {\n",
    "    \"all_correlations\": correlations,\n",
    "    \"mean_correlation\": np.nanmean(correlations),\n",
    "    \"std_correlation\": np.nanstd(correlations),\n",
    "}\n",
    "\n",
    "correlations = []\n",
    "for i in range(y_test.shape[1]):\n",
    "    try:\n",
    "        corr = corrcoef(y_pipeline_pred[:, i], y_test[:, i])\n",
    "        correlations.append(corr)\n",
    "    except:\n",
    "        correlations.append(0.0)\n",
    "\n",
    "results[\"pipeline\"] = {\n",
    "    \"all_correlations\": correlations,\n",
    "    \"mean_correlation\": np.nanmean(correlations),\n",
    "    \"std_correlation\": np.nanstd(correlations),\n",
    "}\n",
    "\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(\n",
    "        f'  Mean correlation: {metrics[\"mean_correlation\"]:.3f} ± {metrics[\"std_correlation\"]:.3f}'\n",
    "    )\n",
    "    print(\n",
    "        f'  Individual correlations: {[f\"{c:.3f}\" for c in metrics[\"all_correlations\"]]}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7694a",
   "metadata": {},
   "source": [
    "### Pipeline from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dbe41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Covariances:\n",
    "    def __init__(self, estimator=\"oas\"):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # X.shape = (n_trials, n_channels, n_samples)\n",
    "        covmats = []\n",
    "        for trial in X:\n",
    "            covmats.append(self._compute_cov(trial))\n",
    "        return np.array(covmats)\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Аналогично fit_transform, если параметры оценки уже заданы.\n",
    "        return self.fit_transform(X)\n",
    "\n",
    "    def _compute_cov(self, X):\n",
    "        \"\"\"\n",
    "        Рассчитывает ковариационную матрицу для одного испытания X (shape: [n_channels, n_samples])\n",
    "        с использованием подхода OAS.\n",
    "        \"\"\"\n",
    "        # Центрируем данные по каждому каналу\n",
    "        Xc = X - np.mean(X, axis=1, keepdims=True)\n",
    "        n_channels, n_samples = Xc.shape\n",
    "\n",
    "        # Стандартная оценка выборочной ковариационной матрицы:\n",
    "        sample_cov = (Xc @ Xc.T) / (n_samples - 1)\n",
    "\n",
    "        if self.estimator.lower() == \"oas\":\n",
    "            # Вычисляем след и сумму квадратов всех элементов (trace_S2)\n",
    "            trace_S = np.trace(sample_cov)\n",
    "            trace_S2 = np.sum(sample_cov**2)\n",
    "\n",
    "            # Вычисление коэффициента shrinkage (α)\n",
    "            shrinkage = min(\n",
    "                1.0,\n",
    "                ((1 - 2.0 / n_channels) * trace_S2 + trace_S**2)\n",
    "                / (\n",
    "                    (n_samples + 1 - 2.0 / n_channels)\n",
    "                    * (trace_S2 - (trace_S**2) / n_channels)\n",
    "                ),\n",
    "            )\n",
    "            # Целевая матрица – диагональная с элементами mu = trace_S / n_channels\n",
    "            mu = trace_S / n_channels\n",
    "            shrunk_cov = (1 - shrinkage) * sample_cov + shrinkage * mu * np.eye(\n",
    "                n_channels\n",
    "            )\n",
    "            return shrunk_cov\n",
    "        else:\n",
    "            # Можно добавить поддержку других оценок. По умолчанию - sample covariance.\n",
    "            return sample_cov\n",
    "\n",
    "\n",
    "def sqrtm(A):\n",
    "    \"\"\"Вычисляет матричный квадратный корень A^(1/2) через собственное разложение.\"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eigh(A)\n",
    "    # Защита от отрицательных значений (ошибки округления)\n",
    "    eigvals = np.maximum(eigvals, 0)\n",
    "    sqrt_eig = np.sqrt(eigvals)\n",
    "    return eigvecs @ np.diag(sqrt_eig) @ eigvecs.T\n",
    "\n",
    "\n",
    "def inv_sqrtm(A):\n",
    "    \"\"\"Вычисляет обратный квадратный корень A^(-1/2) через собственное разложение.\"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eigh(A)\n",
    "    # Избегаем деления на ноль (малые положительные значения)\n",
    "    eigvals = np.maximum(eigvals, 1e-12)\n",
    "    inv_sqrt_eig = 1.0 / np.sqrt(eigvals)\n",
    "    return eigvecs @ np.diag(inv_sqrt_eig) @ eigvecs.T\n",
    "\n",
    "\n",
    "def logm(A):\n",
    "    \"\"\"Вычисляет матричный логарифм лог(A) через собственное разложение.\"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eigh(A)\n",
    "    # Защита от очень малых значений\n",
    "    eigvals = np.maximum(eigvals, 1e-12)\n",
    "    log_eig = np.log(eigvals)\n",
    "    return eigvecs @ np.diag(log_eig) @ eigvecs.T\n",
    "\n",
    "\n",
    "def expm(A):\n",
    "    \"\"\"Вычисляет матричную экспоненту exp(A) через собственное разложение.\"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eigh(A)\n",
    "    exp_eig = np.exp(eigvals)\n",
    "    return eigvecs @ np.diag(exp_eig) @ eigvecs.T\n",
    "\n",
    "\n",
    "def riemannian_mean(covs, tol=1e-6, max_iter=50):\n",
    "    \"\"\"\n",
    "    Вычисляет Риманово среднее (геометрическое среднее) ковариационных матриц.\n",
    "    covs: массив ковариационных матриц (n_matrices, n_channels, n_channels)\n",
    "    \"\"\"\n",
    "    n_matrices = covs.shape[0]\n",
    "    # Инициализируем арифметическим средним\n",
    "    M = np.mean(covs, axis=0)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        inv_sqrt_M = inv_sqrtm(M)\n",
    "        logs = np.zeros_like(M)\n",
    "        for C in covs:\n",
    "            T = logm(inv_sqrt_M @ C @ inv_sqrt_M)\n",
    "            logs += T\n",
    "        logs /= n_matrices\n",
    "        M_new = sqrtm(M) @ expm(logs) @ sqrtm(M)\n",
    "        # Проверка сходимости по норме Фробениуса\n",
    "        if np.linalg.norm(M_new - M, ord=\"fro\") / np.linalg.norm(M, ord=\"fro\") < tol:\n",
    "            M = M_new\n",
    "            break\n",
    "        M = M_new\n",
    "    return M\n",
    "\n",
    "\n",
    "class TangentSpace:\n",
    "    def __init__(self, metric=\"riemann\"):\n",
    "        self.metric = metric\n",
    "        self.ref_ = None\n",
    "\n",
    "    def fit_transform(self, covs):\n",
    "        \"\"\"\n",
    "        covs: массив ковариационных матриц размера (n_matrices, n_channels, n_channels)\n",
    "        \"\"\"\n",
    "        if self.metric == \"riemann\":\n",
    "            self.ref_ = riemannian_mean(covs)\n",
    "        else:\n",
    "            # По умолчанию арифметическое среднее\n",
    "            self.ref_ = np.mean(covs, axis=0)\n",
    "        return self.transform(covs)\n",
    "\n",
    "    def transform(self, covs):\n",
    "        \"\"\"\n",
    "        Проецирует каждую ковариационную матрицу в тангенциальное пространство.\n",
    "        Возвращает массив векторизованных форм проекций.\n",
    "        \"\"\"\n",
    "        inv_sqrt_ref = inv_sqrtm(self.ref_)\n",
    "        n_matrices = covs.shape[0]\n",
    "        n_channels = covs.shape[1]\n",
    "        ts = []\n",
    "        for C in covs:\n",
    "            # Вычисляем матрицу: T = log( ref^{-1/2} * C * ref^{-1/2} )\n",
    "            T = logm(inv_sqrt_ref @ C @ inv_sqrt_ref)\n",
    "            # Векторизуем симметричную матрицу: выбираем верхнетреугольные элементы (включая диагональ)\n",
    "            vec = T[np.triu_indices(n_channels)]\n",
    "            ts.append(vec)\n",
    "        return np.array(ts)\n",
    "\n",
    "\n",
    "class RiemannMLPRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=\"oas\",\n",
    "        metric=\"riemann\",\n",
    "        hidden_layer_sizes=(200,),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-5,\n",
    "        max_iter=200,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.estimator = estimator\n",
    "        self.metric = metric\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # Внутренние объекты моделей\n",
    "        self.cov_ = None\n",
    "        self.ts_ = None\n",
    "        self.reg_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 1. Вычисление ковариационных матриц\n",
    "        self.cov_ = Covariances(estimator=self.estimator)\n",
    "        X_cov = self.cov_.fit_transform(X)\n",
    "\n",
    "        # 2. Проекция в тангенциальное пространство\n",
    "        self.ts_ = TangentSpace(metric=self.metric)\n",
    "        X_ts = self.ts_.fit_transform(X_cov)\n",
    "\n",
    "        # 3. MLP Regressor\n",
    "        self.reg_ = MLPRegressor(\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver=self.solver,\n",
    "            alpha=self.alpha,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state,\n",
    "        )\n",
    "        self.reg_.fit(X_ts, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Применение тех же преобразований к новым данным\n",
    "        X_cov = self.cov_.transform(X)\n",
    "        X_ts = self.ts_.transform(X_cov)\n",
    "        return self.reg_.predict(X_ts)\n",
    "\n",
    "\n",
    "# Пример использования:\n",
    "# Пусть X_train имеет размер (3195, 6, 250), y_train – (3195, 20)\n",
    "np.random.seed(42)\n",
    "# X_train = np.random.randn(3195, 6, 250)\n",
    "# y_train = np.random.randn(3195, 20)\n",
    "\n",
    "model = RiemannMLPRegressor(\n",
    "    estimator=\"oas\",\n",
    "    metric=\"riemann\",\n",
    "    hidden_layer_sizes=(200,),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=1e-5,\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_model_pred = model.predict(X_test)\n",
    "\n",
    "results = {}\n",
    "correlations = []\n",
    "for i in range(y_test.shape[1]):\n",
    "    try:\n",
    "        corr = corrcoef(y_model_pred[:, i], y_test[:, i])\n",
    "        correlations.append(corr)\n",
    "    except:\n",
    "        correlations.append(0.0)\n",
    "\n",
    "results[\"model\"] = {\n",
    "    \"all_correlations\": correlations,\n",
    "    \"mean_correlation\": np.nanmean(correlations),\n",
    "    \"std_correlation\": np.nanstd(correlations),\n",
    "}\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(\n",
    "        f'  Mean correlation: {metrics[\"mean_correlation\"]:.3f} ± {metrics[\"std_correlation\"]:.3f}'\n",
    "    )\n",
    "    print(\n",
    "        f'  Individual correlations: {[f\"{c:.3f}\" for c in metrics[\"all_correlations\"]]}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6e820",
   "metadata": {},
   "source": [
    "### Pipeline and torch mlp block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9aac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layer_sizes, output_dim, activation=\"relu\"):\n",
    "        super(TorchMLP, self).__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        # nn.GELU, nn.Softshrink\n",
    "        act_layer = nn.GELU\n",
    "        for h in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(current_dim, h))\n",
    "            layers.append(act_layer())\n",
    "            current_dim = h\n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class RiemannMLPRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=\"oas\",\n",
    "        metric=\"riemann\",\n",
    "        hidden_layer_sizes=(200,),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-5,\n",
    "        max_iter=200,\n",
    "        random_state=None,\n",
    "        device=\"cpu\",\n",
    "        batch_size=64,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.estimator = estimator\n",
    "        self.metric = metric\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.cov_ = None\n",
    "        self.ts_ = None\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Ковариации и тангенциальное пространство\n",
    "        self.cov_ = Covariances(estimator=self.estimator)\n",
    "        X_cov = self.cov_.fit_transform(X)\n",
    "        self.ts_ = TangentSpace(metric=self.metric)\n",
    "        X_ts = self.ts_.fit_transform(X_cov)\n",
    "\n",
    "        print(\"X_cov.shape:\", X_cov.shape)\n",
    "        print(\"X_ts.shape:\", X_ts.shape)\n",
    "\n",
    "        X_ts = torch.tensor(X_ts, dtype=torch.float32).to(self.device)\n",
    "        y = torch.tensor(y, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        input_dim = X_ts.shape[1]\n",
    "        output_dim = y.shape[1]\n",
    "\n",
    "        self.model_ = TorchMLP(\n",
    "            input_dim=input_dim,\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            output_dim=output_dim,\n",
    "            activation=self.activation,\n",
    "        ).to(self.device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(\n",
    "            self.model_.parameters(), lr=0.001, weight_decay=self.alpha\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X_ts, y)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "        for epoch in range(self.max_iter):\n",
    "            self.model_.train()\n",
    "            epoch_loss = 0\n",
    "            for X_batch, y_batch in loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model_(X_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            avg_loss = epoch_loss / len(loader)\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.max_iter}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_cov = self.cov_.transform(X)\n",
    "        X_ts = self.ts_.transform(X_cov)\n",
    "        X_ts = torch.tensor(X_ts, dtype=torch.float32).to(self.device)\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model_(X_ts).cpu().numpy()\n",
    "        return preds\n",
    "\n",
    "\n",
    "model = RiemannMLPRegressor(\n",
    "    estimator=\"oas\",\n",
    "    metric=\"riemann\",\n",
    "    hidden_layer_sizes=(400, 400),\n",
    "    activation=\"relu\",\n",
    "    max_iter=1000,\n",
    "    alpha=1e-5,\n",
    "    random_state=42,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_model_pred = model.predict(X_test)\n",
    "\n",
    "results = {}\n",
    "correlations = []\n",
    "for i in range(y_test.shape[1]):\n",
    "    try:\n",
    "        corr = corrcoef(y_model_pred[:, i], y_test[:, i])\n",
    "        correlations.append(corr)\n",
    "    except:\n",
    "        correlations.append(0.0)\n",
    "\n",
    "results[\"model\"] = {\n",
    "    \"all_correlations\": correlations,\n",
    "    \"mean_correlation\": np.nanmean(correlations),\n",
    "    \"std_correlation\": np.nanstd(correlations),\n",
    "}\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(\n",
    "        f'  Mean correlation: {metrics[\"mean_correlation\"]:.3f} ± {metrics[\"std_correlation\"]:.3f}'\n",
    "    )\n",
    "    print(\n",
    "        f'  Individual correlations: {[f\"{c:.3f}\" for c in metrics[\"all_correlations\"]]}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc1e7195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cov.shape: (1599, 6, 6)\n",
      "X_ts.shape: (1599, 21)\n",
      "Epoch 50/300: Train Loss: 0.0062, Train Corr: 0.962, Val Loss: 0.0122, Val Corr: 0.939\n",
      "Epoch 100/300: Train Loss: 0.0043, Train Corr: 0.972, Val Loss: 0.0117, Val Corr: 0.944\n",
      "Epoch 150/300: Train Loss: 0.0036, Train Corr: 0.976, Val Loss: 0.0119, Val Corr: 0.946\n",
      "Epoch 200/300: Train Loss: 0.0030, Train Corr: 0.979, Val Loss: 0.0117, Val Corr: 0.948\n",
      "Epoch 250/300: Train Loss: 0.0026, Train Corr: 0.981, Val Loss: 0.0120, Val Corr: 0.949\n",
      "Epoch 300/300: Train Loss: 0.0025, Train Corr: 0.981, Val Loss: 0.0124, Val Corr: 0.948\n",
      "model:\n",
      "  Mean correlation: 0.948 ± 0.021\n",
      "  Individual correlations: ['0.965', '0.908', '0.969', '0.971', '0.946', '0.915', '0.956', '0.960', '0.951', '0.962', '0.952', '0.956', '0.952', '0.884', '0.952', '0.956', '0.947', '0.959', '0.950', '0.958']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def corrcoef(x, y):\n",
    "    if np.std(x) == 0 or np.std(y) == 0:\n",
    "        return 0.0\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "\n",
    "# class TorchMLP(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_layer_sizes, output_dim, activation=\"relu\"):\n",
    "#         super(TorchMLP, self).__init__()\n",
    "#         layers = []\n",
    "#         current_dim = input_dim\n",
    "#         act_layer = nn.GELU  # Можно подставить nn.ReLU() или другую\n",
    "#         for h in hidden_layer_sizes:\n",
    "#             layers.append(nn.Linear(current_dim, h))\n",
    "#             layers.append(act_layer())\n",
    "#             current_dim = h\n",
    "#         layers.append(nn.Linear(current_dim, output_dim))\n",
    "#         self.model = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_layer_sizes: list,\n",
    "        output_dim: int,\n",
    "        activation: str = \"gelu\",\n",
    "        dropout_rate: float = 0.2,\n",
    "        use_bn: bool = True,\n",
    "        residual_connections: bool = False,\n",
    "        init_method: str = \"he\",\n",
    "    ):\n",
    "        super(TorchMLP, self).__init__()\n",
    "        self.residual_connections = residual_connections\n",
    "\n",
    "        # Выбор активации\n",
    "        activation_dict = {\n",
    "            \"relu\": nn.ReLU,\n",
    "            \"gelu\": nn.GELU,\n",
    "            \"selu\": nn.SELU,\n",
    "            \"leaky_relu\": nn.LeakyReLU,\n",
    "            \"swish\": nn.SiLU,\n",
    "        }\n",
    "        act_layer = activation_dict.get(activation.lower(), nn.GELU)\n",
    "\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "\n",
    "        # Построение скрытых слоев\n",
    "        for i, h in enumerate(hidden_layer_sizes):\n",
    "            # Линейный слой\n",
    "            layers.append(nn.Linear(current_dim, h))\n",
    "\n",
    "            # Инициализация весов\n",
    "            if init_method == \"he\":\n",
    "                nn.init.kaiming_normal_(layers[-1].weight, nonlinearity=\"relu\")\n",
    "            elif init_method == \"xavier\":\n",
    "                nn.init.xavier_normal_(layers[-1].weight)\n",
    "\n",
    "            # Batch Normalization\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "\n",
    "            # Активация\n",
    "            layers.append(act_layer())\n",
    "\n",
    "            # Dropout\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "            # Обновление размерности\n",
    "            current_dim = h\n",
    "\n",
    "        # Выходной слой\n",
    "        self.output_layer = nn.Linear(current_dim, output_dim)\n",
    "\n",
    "        # Инициализация выходного слоя\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "        nn.init.constant_(self.output_layer.bias, 0)\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear) and self.residual_connections:\n",
    "                if x.shape[-1] == layer.out_features:\n",
    "                    residual = x\n",
    "                x = layer(x)\n",
    "                if x.shape == residual.shape:\n",
    "                    x += residual\n",
    "                    residual = x\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class RiemannMLPRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=\"oas\",\n",
    "        metric=\"riemann\",\n",
    "        hidden_layer_sizes=(200,),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-5,\n",
    "        max_iter=200,\n",
    "        random_state=None,\n",
    "        device=\"cpu\",\n",
    "        batch_size=64,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.estimator = estimator\n",
    "        self.metric = metric\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.cov_ = None\n",
    "        self.ts_ = None\n",
    "        self.model_ = None\n",
    "\n",
    "    # Изменённый метод fit принимает дополнительно X_val и y_val для вычисления тестового лосса и корреляции\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        # Вычисление ковариаций и переход в тангенциальное пространство\n",
    "        self.cov_ = Covariances(estimator=self.estimator)\n",
    "        X_cov = self.cov_.fit_transform(X)\n",
    "        self.ts_ = TangentSpace(metric=self.metric)\n",
    "        X_ts = self.ts_.fit_transform(X_cov)\n",
    "\n",
    "        print(\"X_cov.shape:\", X_cov.shape)\n",
    "        print(\"X_ts.shape:\", X_ts.shape)\n",
    "\n",
    "        X_ts = torch.tensor(X_ts, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        input_dim = X_ts.shape[1]\n",
    "        output_dim = y_tensor.shape[1]\n",
    "\n",
    "        # self.model_ = TorchMLP(\n",
    "        #     input_dim=input_dim,\n",
    "        #     hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "        #     output_dim=output_dim,\n",
    "        #     activation=self.activation,\n",
    "        # ).to(self.device)\n",
    "\n",
    "        self.model_ = TorchMLP(\n",
    "            input_dim=input_dim,\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            output_dim=output_dim,\n",
    "            activation=self.activation,\n",
    "            dropout_rate=0,\n",
    "            use_bn=False,\n",
    "            residual_connections=False,\n",
    "            init_method=\"no\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(\n",
    "            self.model_.parameters(), lr=0.001, weight_decay=self.alpha\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.7)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X_ts, y_tensor)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "        # Если заданы данные для валидации, подготовим их\n",
    "        if (X_val is not None) and (y_val is not None):\n",
    "            X_cov_val = self.cov_.transform(X_val)\n",
    "            X_ts_val = self.ts_.transform(X_cov_val)\n",
    "            X_ts_val = torch.tensor(X_ts_val, dtype=torch.float32).to(self.device)\n",
    "            y_tensor_val = torch.tensor(y_val, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Обучение модели\n",
    "        for epoch in range(self.max_iter):\n",
    "            self.model_.train()\n",
    "            epoch_loss = 0\n",
    "            for X_batch, y_batch in loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model_(X_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            # Каждые 50 эпох (или можно менять условие) вычисляем и выводим метрики\n",
    "            if self.verbose and ((epoch + 1) % 50 == 0):\n",
    "                # Рассчёт метрик на тренировочных данных\n",
    "                self.model_.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Предсказания на тренинге\n",
    "                    train_preds = self.model_(X_ts).cpu().numpy()\n",
    "                    train_loss = criterion(self.model_(X_ts), y_tensor).item()\n",
    "\n",
    "                    # Вычисление корреляции по каждому выходу\n",
    "                    train_corrs = []\n",
    "                    y_true_np = y_tensor.cpu().numpy()\n",
    "                    for i in range(train_preds.shape[1]):\n",
    "                        train_corrs.append(corrcoef(train_preds[:, i], y_true_np[:, i]))\n",
    "                    train_mean_corr = np.nanmean(train_corrs)\n",
    "\n",
    "                    metrics_str = f\"Epoch {epoch+1}/{self.max_iter}: Train Loss: {train_loss:.4f}, Train Corr: {train_mean_corr:.3f}\"\n",
    "\n",
    "                    # Если есть валидация – аналогичные расчёты\n",
    "                    if (X_val is not None) and (y_val is not None):\n",
    "                        val_preds = self.model_(X_ts_val).cpu().numpy()\n",
    "                        val_loss = criterion(self.model_(X_ts_val), y_tensor_val).item()\n",
    "                        val_corrs = []\n",
    "                        y_val_np = y_tensor_val.cpu().numpy()\n",
    "                        for i in range(val_preds.shape[1]):\n",
    "                            val_corrs.append(corrcoef(val_preds[:, i], y_val_np[:, i]))\n",
    "                        val_mean_corr = np.nanmean(val_corrs)\n",
    "                        metrics_str += (\n",
    "                            f\", Val Loss: {val_loss:.4f}, Val Corr: {val_mean_corr:.3f}\"\n",
    "                        )\n",
    "\n",
    "                    print(metrics_str)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_cov = self.cov_.transform(X)\n",
    "        X_ts = self.ts_.transform(X_cov)\n",
    "        X_ts = torch.tensor(X_ts, dtype=torch.float32).to(self.device)\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model_(X_ts).cpu().numpy()\n",
    "        return preds\n",
    "\n",
    "\n",
    "model = RiemannMLPRegressor(\n",
    "    estimator=\"oas\",\n",
    "    metric=\"riemann\",\n",
    "    hidden_layer_sizes=(128, 128),\n",
    "    activation=\"gelu\",\n",
    "    max_iter=300,\n",
    "    alpha=1e-5,\n",
    "    random_state=42,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, X_test, y_test)\n",
    "y_model_pred = model.predict(X_test)\n",
    "\n",
    "results = {}\n",
    "correlations = []\n",
    "for i in range(y_test.shape[1]):\n",
    "    try:\n",
    "        corr = corrcoef(y_model_pred[:, i], y_test[:, i])\n",
    "        correlations.append(corr)\n",
    "    except:\n",
    "        correlations.append(0.0)\n",
    "\n",
    "results[\"model\"] = {\n",
    "    \"all_correlations\": correlations,\n",
    "    \"mean_correlation\": np.nanmean(correlations),\n",
    "    \"std_correlation\": np.nanstd(correlations),\n",
    "}\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(\n",
    "        f'  Mean correlation: {metrics[\"mean_correlation\"]:.3f} ± {metrics[\"std_correlation\"]:.3f}'\n",
    "    )\n",
    "    print(\n",
    "        f'  Individual correlations: {[f\"{c:.3f}\" for c in metrics[\"all_correlations\"]]}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fb6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.plot(y_test[:, i])\n",
    "    plt.plot(y_model_pred[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbfe882",
   "metadata": {},
   "source": [
    "### History prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "subj_path = \"..\\\\data\\\\Fedor\\\\Raw\\\\preproc_angles\\\\1\\\\\"\n",
    "fs = 500\n",
    "\n",
    "\n",
    "def corrcoef(x, y):\n",
    "    if np.std(x) == 0 or np.std(y) == 0:\n",
    "        return 0.0\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "\n",
    "def train_test_split(data, N_parts, num_of_part):\n",
    "    N_samples = len(data)\n",
    "\n",
    "    l_idx = int((N_samples * num_of_part) / N_parts)\n",
    "    h_idx = int((N_samples * (num_of_part + 1)) / N_parts)\n",
    "\n",
    "    data_train = np.concatenate([data[:l_idx, :], data[h_idx:,]], axis=0)\n",
    "    data_test = data[l_idx:h_idx, :]\n",
    "\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "N_files = 4\n",
    "data_list_train = list()\n",
    "data_list_test = list()\n",
    "label_list_train = list()\n",
    "label_list_test = list()\n",
    "\n",
    "N_parts = 10\n",
    "num_of_part = 9\n",
    "\n",
    "for i in range(N_files):\n",
    "\n",
    "    arr = np.load(subj_path + \"000\" + str(i) + \".npz\")\n",
    "\n",
    "    std_coef = arr[\"std_coef\"]\n",
    "    data = arr[\"data_myo\"]\n",
    "    label = arr[\"data_angles\"]\n",
    "\n",
    "    data_train, data_test = train_test_split(data, N_parts, num_of_part)\n",
    "    label_train, label_test = train_test_split(label, N_parts, num_of_part)\n",
    "\n",
    "    data_list_train.append(data_train)\n",
    "    data_list_test.append(data_test)\n",
    "\n",
    "    label_list_train.append(label_train)\n",
    "    label_list_test.append(label_test)\n",
    "\n",
    "\n",
    "data_train = np.concatenate(data_list_train, axis=0)\n",
    "data_test = np.concatenate(data_list_test, axis=0)\n",
    "\n",
    "\n",
    "label_train = np.concatenate(label_list_train, axis=0)\n",
    "label_test = np.concatenate(label_list_test, axis=0)\n",
    "\n",
    "\n",
    "def slicer(data, label, fs, windowlen=500, timestep=100):\n",
    "    data_len = len(data)\n",
    "    timestep_samples = int((timestep * fs) / 1000)\n",
    "    windowlen_samples = int((windowlen * fs) / 1000)\n",
    "\n",
    "    start_idc = np.arange(0, data_len - windowlen_samples, timestep_samples)[:, None]\n",
    "    window_idc = np.arange(0, windowlen_samples)[None, :]\n",
    "    slice_idc = start_idc + window_idc\n",
    "\n",
    "    slice_data = data[slice_idc].transpose(0, 2, 1)\n",
    "    slice_label = label[start_idc[:, 0] + windowlen_samples]\n",
    "    return slice_data, slice_label\n",
    "\n",
    "\n",
    "def slicer_sequence(data, label, fs, windowlen=500, timestep=100, seq_len=5):\n",
    "    \"\"\"\n",
    "    Формирует последовательности для каждого окна EMG-сигнала, извлекая также историю предыдущих углов движения.\n",
    "    Возвращает:\n",
    "      - slice_data: исходные данные EMG для обработки ковариациями\n",
    "      - prev_angles: последовательность (конкатенация) предыдущих углов\n",
    "      - slice_label: целевое значение углов (следующий момент после окна)\n",
    "    \"\"\"\n",
    "    data_len = len(data)\n",
    "    timestep_samples = int((timestep * fs) / 1000)\n",
    "    windowlen_samples = int((windowlen * fs) / 1000)\n",
    "\n",
    "    # Индексы стартовых точек, начиная с момента, когда уже накоплена история seq_len\n",
    "    start_indices = np.arange(\n",
    "        seq_len * timestep_samples, data_len - windowlen_samples, timestep_samples\n",
    "    )\n",
    "\n",
    "    slice_data = []\n",
    "    prev_angles = []\n",
    "    slice_label = []\n",
    "\n",
    "    for idx in start_indices:\n",
    "        # Извлекаем историю: собираем значения углов за seq_len предыдущих шагов\n",
    "        history = []\n",
    "        for i in range(seq_len):\n",
    "            offset = idx - (seq_len - i) * timestep_samples\n",
    "            # Можно взять значение углов сразу после соответствующего окна\n",
    "            history.append(label[offset + windowlen_samples])\n",
    "        # Объединяем историю в единый вектор\n",
    "        history = np.concatenate(history, axis=0)\n",
    "        prev_angles.append(history)\n",
    "\n",
    "        # Извлекаем EMG-сигнал за текущее окно\n",
    "        window_idx = np.arange(idx, idx + windowlen_samples)\n",
    "        slice_data.append(data[window_idx, :])\n",
    "        # Целевая метка – углы движения сразу после окна\n",
    "        slice_label.append(label[idx + windowlen_samples])\n",
    "\n",
    "    return (\n",
    "        np.array(slice_data).transpose(0, 2, 1),\n",
    "        np.array(prev_angles),\n",
    "        np.array(slice_label),\n",
    "    )\n",
    "\n",
    "\n",
    "# X_train, y_train = slicer(data_train, label_train, fs)\n",
    "# X_test, y_test = slicer(data_test, label_test, fs)\n",
    "\n",
    "X_train_emg, X_train_history, y_train = slicer_sequence(\n",
    "    data_train, label_train, fs, seq_len=5\n",
    ")\n",
    "X_test_emg, X_test_history, y_test = slicer_sequence(\n",
    "    data_test, label_test, fs, seq_len=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b263e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Простейший MLP для объединения признаков из Тангенциального пространства и истории углов\n",
    "class DualInputMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ts_input_dim,\n",
    "        history_dim,\n",
    "        hidden_sizes=(400, 400),\n",
    "        output_dim=20,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super(DualInputMLP, self).__init__()\n",
    "        combined_dim = ts_input_dim + history_dim  # В нашем случае 21 + (seq_len * 20)\n",
    "        layers = []\n",
    "        current_dim = combined_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(current_dim, h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            current_dim = h\n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, ts_features, history_features):\n",
    "        # Конкатенация признаков из тангенциального пространства и истории\n",
    "        x = torch.cat((ts_features, history_features), dim=1)\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RiemannMLPRegressorWithHistory(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=\"oas\",\n",
    "        metric=\"riemann\",\n",
    "        hidden_layer_sizes=(400, 400),\n",
    "        alpha=1e-5,\n",
    "        max_iter=100,\n",
    "        random_state=42,\n",
    "        device=\"cpu\",\n",
    "        batch_size=64,\n",
    "        verbose=False,\n",
    "        seq_len=5,  # Количество предыдущих шагов для истории углов\n",
    "    ):\n",
    "        self.estimator = estimator\n",
    "        self.metric = metric\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.cov_ = None\n",
    "        self.ts_ = None\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X_emg, X_history, y):\n",
    "        \"\"\"\n",
    "        X_emg: массив с исходными срезами ЭМГ-сигнала (например, размер (3195, windowlen_samples, n_channels))\n",
    "        X_history: массив с векторами истории углов (например, размер (3195, seq_len*20))\n",
    "        y: целевые значения углов (например, размер (3195, 20))\n",
    "        \"\"\"\n",
    "        # 1. Преобразование ЭМГ-сигнала через ковариации и тангенциальное пространство\n",
    "        self.cov_ = Covariances(estimator=self.estimator)\n",
    "        X_cov = self.cov_.fit_transform(\n",
    "            X_emg\n",
    "        )  # X_cov.shape должно быть (N_samples, 6, 6)\n",
    "\n",
    "        self.ts_ = TangentSpace(metric=self.metric)\n",
    "        X_ts = self.ts_.fit_transform(X_cov)  # X_ts.shape будет (N_samples, 21)\n",
    "        print(\"X_cov.shape:\", X_cov.shape)\n",
    "        print(\"X_ts.shape:\", X_ts.shape)\n",
    "\n",
    "        # Преобразуем данные в тензоры\n",
    "        X_ts_tensor = torch.tensor(X_ts, dtype=torch.float32).to(self.device)\n",
    "        history_tensor = torch.tensor(X_history, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        ts_input_dim = X_ts_tensor.shape[1]  # 21\n",
    "        history_dim = history_tensor.shape[1]\n",
    "        output_dim = y_tensor.shape[1]  # 20\n",
    "\n",
    "        self.model_ = DualInputMLP(\n",
    "            ts_input_dim, history_dim, self.hidden_layer_sizes, output_dim\n",
    "        ).to(self.device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(\n",
    "            self.model_.parameters(), lr=0.001, weight_decay=self.alpha\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X_ts_tensor, history_tensor, y_tensor)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "        for epoch in range(self.max_iter):\n",
    "            self.model_.train()\n",
    "            epoch_loss = 0\n",
    "            for ts_batch, history_batch, y_batch in loader:\n",
    "                optimizer.zero_grad()\n",
    "                preds = self.model_(ts_batch, history_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.max_iter}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def val(self, X_emg, X_history):\n",
    "        X_cov = self.cov_.transform(X_emg)\n",
    "        X_ts = self.ts_.transform(X_cov)\n",
    "\n",
    "        X_ts_tensor = torch.tensor(X_ts, dtype=torch.float32).to(self.device)\n",
    "        history_tensor = torch.tensor(X_history, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model_(X_ts_tensor, history_tensor).cpu().numpy()\n",
    "        return preds\n",
    "\n",
    "    def predict(self, X_emg, initial_history=None):\n",
    "        \"\"\"\n",
    "        Авто-регрессионный инференс:\n",
    "        - X_emg: массив с срезами ЭМГ-сигнала, например (N_samples, windowlen_samples, n_channels)\n",
    "        - initial_history: начальное значение истории, размер (seq_len, output_dim).\n",
    "            Если не передано, используется нулевая инициализация.\n",
    "        Возвращает:\n",
    "        - numpy-массив предсказаний, размер (N_samples, output_dim)\n",
    "        \"\"\"\n",
    "        # Определяем выходную размерность (число углов). Предполагаем, что последний слой модели имеет атрибут out_features.\n",
    "        output_dim = self.model_.model[-1].out_features\n",
    "\n",
    "        # Если начальная история не задана, инициализируем её нулями\n",
    "        if initial_history is None:\n",
    "            history_vec = np.zeros((self.seq_len, output_dim))\n",
    "        else:\n",
    "            # Если передана история, приводим её к нужному виду\n",
    "            history_vec = np.array(initial_history).reshape(self.seq_len, output_dim)\n",
    "\n",
    "        predictions = []\n",
    "        num_samples = X_emg.shape[0]\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # Извлекаем текущее окно ЭМГ (форма: [windowlen_samples, n_channels])\n",
    "            current_emg = X_emg[i]\n",
    "\n",
    "            # Преобразуем текущее окно через Covariances и TangentSpace:\n",
    "            X_cov = self.cov_.transform(\n",
    "                np.expand_dims(current_emg, axis=0)\n",
    "            )  # форма: (1, 6, 6)\n",
    "            X_ts = self.ts_.transform(X_cov)  # форма: (1, 21)\n",
    "\n",
    "            # Формируем историю в виде вектора: преобразуем массив history_vec (seq_len, output_dim)\n",
    "            # в одномерный вектор размерности seq_len*output_dim и добавляем размерность пакета (1, ...)\n",
    "            history_input = history_vec.flatten()[\n",
    "                np.newaxis, :\n",
    "            ]  # форма: (1, seq_len*output_dim)\n",
    "\n",
    "            # Приводим данные к тензорам PyTorch:\n",
    "            ts_tensor = torch.tensor(X_ts, dtype=torch.float32).to(self.device)\n",
    "            hist_tensor = torch.tensor(history_input, dtype=torch.float32).to(\n",
    "                self.device\n",
    "            )\n",
    "\n",
    "            # Выполняем предсказание:\n",
    "            self.model_.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_tensor = self.model_(ts_tensor, hist_tensor)\n",
    "            pred = pred_tensor.cpu().numpy()[0]\n",
    "\n",
    "            predictions.append(pred)\n",
    "\n",
    "            # Обновляем историю:\n",
    "            # Сдвигаем историю на один шаг влево и ставим новое предсказание в конец\n",
    "            history_vec = np.roll(history_vec, shift=-1, axis=0)\n",
    "            history_vec[-1, :] = pred\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "model = RiemannMLPRegressorWithHistory(\n",
    "    estimator=\"oas\",\n",
    "    metric=\"riemann\",\n",
    "    hidden_layer_sizes=(400, 400),\n",
    "    max_iter=1000,\n",
    "    alpha=1e-5,\n",
    "    random_state=42,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    verbose=True,\n",
    "    seq_len=5,\n",
    ")\n",
    "\n",
    "model.fit(X_train_emg, X_train_history, y_train)\n",
    "y_model_pred = model.val(X_test_emg, X_test_history)\n",
    "\n",
    "results = {}\n",
    "correlations = []\n",
    "for i in range(y_test.shape[1]):\n",
    "    try:\n",
    "        corr = corrcoef(y_model_pred[:, i], y_test[:, i])\n",
    "        correlations.append(corr)\n",
    "    except:\n",
    "        correlations.append(0.0)\n",
    "\n",
    "results[\"model\"] = {\n",
    "    \"all_correlations\": correlations,\n",
    "    \"mean_correlation\": np.nanmean(correlations),\n",
    "    \"std_correlation\": np.nanstd(correlations),\n",
    "}\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(\n",
    "        f'  Mean correlation: {metrics[\"mean_correlation\"]:.3f} ± {metrics[\"std_correlation\"]:.3f}'\n",
    "    )\n",
    "    print(\n",
    "        f'  Individual correlations: {[f\"{c:.3f}\" for c in metrics[\"all_correlations\"]]}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model_pred = model.predict(X_test_emg)\n",
    "\n",
    "results = {}\n",
    "correlations = []\n",
    "for i in range(y_test.shape[1]):\n",
    "    try:\n",
    "        corr = corrcoef(y_model_pred[:, i], y_test[:, i])\n",
    "        correlations.append(corr)\n",
    "    except:\n",
    "        correlations.append(0.0)\n",
    "\n",
    "results[\"model\"] = {\n",
    "    \"all_correlations\": correlations,\n",
    "    \"mean_correlation\": np.nanmean(correlations),\n",
    "    \"std_correlation\": np.nanstd(correlations),\n",
    "}\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(\n",
    "        f'  Mean correlation: {metrics[\"mean_correlation\"]:.3f} ± {metrics[\"std_correlation\"]:.3f}'\n",
    "    )\n",
    "    print(\n",
    "        f'  Individual correlations: {[f\"{c:.3f}\" for c in metrics[\"all_correlations\"]]}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bab203",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.plot(y_test[:, i])\n",
    "    plt.plot(y_model_pred[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3af601e",
   "metadata": {},
   "source": [
    "#### Autoregressive Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Код подготовки данных\n",
    "# ----------------------------\n",
    "subj_path = \"..\\\\data\\\\Fedor\\\\Raw\\\\preproc_angles\\\\1\\\\\"\n",
    "fs = 500\n",
    "\n",
    "\n",
    "def corrcoef(x, y):\n",
    "    if np.std(x) == 0 or np.std(y) == 0:\n",
    "        return 0.0\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "\n",
    "def train_test_split(data, N_parts, num_of_part):\n",
    "    N_samples = len(data)\n",
    "\n",
    "    l_idx = int((N_samples * num_of_part) / N_parts)\n",
    "    h_idx = int((N_samples * (num_of_part + 1)) / N_parts)\n",
    "\n",
    "    data_train = np.concatenate([data[:l_idx, :], data[h_idx:,]], axis=0)\n",
    "    data_test = data[l_idx:h_idx, :]\n",
    "\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "N_files = 4\n",
    "data_list_train = list()\n",
    "data_list_test = list()\n",
    "label_list_train = list()\n",
    "label_list_test = list()\n",
    "\n",
    "N_parts = 10\n",
    "num_of_part = 9\n",
    "\n",
    "for i in range(N_files):\n",
    "    arr = np.load(subj_path + \"000\" + str(i) + \".npz\")\n",
    "    std_coef = arr[\"std_coef\"]\n",
    "    data = arr[\"data_myo\"]\n",
    "    label = arr[\"data_angles\"]\n",
    "\n",
    "    data_train, data_test = train_test_split(data, N_parts, num_of_part)\n",
    "    label_train, label_test = train_test_split(label, N_parts, num_of_part)\n",
    "\n",
    "    data_list_train.append(data_train)\n",
    "    data_list_test.append(data_test)\n",
    "    label_list_train.append(label_train)\n",
    "    label_list_test.append(label_test)\n",
    "\n",
    "data_train = np.concatenate(data_list_train, axis=0)\n",
    "data_test = np.concatenate(data_list_test, axis=0)\n",
    "label_train = np.concatenate(label_list_train, axis=0)\n",
    "label_test = np.concatenate(label_list_test, axis=0)\n",
    "\n",
    "\n",
    "def slicer_sequence(data, label, fs, windowlen=500, timestep=100, seq_len=5):\n",
    "    \"\"\"\n",
    "    Формирует последовательности для каждого окна EMG-сигнала, извлекая также историю предыдущих углов движения.\n",
    "    Возвращает:\n",
    "      - slice_data: исходные данные EMG для обработки ковариациями\n",
    "      - prev_angles: последовательность (конкатенация) предыдущих углов (ground truth; используется для начальной инициализации)\n",
    "      - slice_label: целевое значение углов (следующий момент после окна)\n",
    "    \"\"\"\n",
    "    data_len = len(data)\n",
    "    timestep_samples = int((timestep * fs) / 1000)\n",
    "    windowlen_samples = int((windowlen * fs) / 1000)\n",
    "\n",
    "    # Индексы стартовых точек, начиная с момента, когда уже накоплена история seq_len\n",
    "    start_indices = np.arange(\n",
    "        seq_len * timestep_samples, data_len - windowlen_samples, timestep_samples\n",
    "    )\n",
    "\n",
    "    slice_data = []\n",
    "    prev_angles = []\n",
    "    slice_label = []\n",
    "\n",
    "    for idx in start_indices:\n",
    "        # Извлекаем историю: собираем значения углов за seq_len предыдущих шагов\n",
    "        history = []\n",
    "        for i in range(seq_len):\n",
    "            offset = idx - (seq_len - i) * timestep_samples\n",
    "            history.append(label[offset + windowlen_samples])\n",
    "        # Объединяем историю в единый вектор\n",
    "        history = np.concatenate(history, axis=0)\n",
    "        prev_angles.append(history)\n",
    "\n",
    "        # Извлекаем EMG-сигнал за текущее окно\n",
    "        window_idx = np.arange(idx, idx + windowlen_samples)\n",
    "        slice_data.append(data[window_idx, :])\n",
    "        # Целевая метка – углы движения сразу после окна\n",
    "        slice_label.append(label[idx + windowlen_samples])\n",
    "\n",
    "    # Транспонируем slice_data, чтобы получить форму (N_samples, n_channels, windowlen_samples)\n",
    "    return (\n",
    "        np.array(slice_data).transpose(0, 2, 1),\n",
    "        np.array(prev_angles),\n",
    "        np.array(slice_label),\n",
    "    )\n",
    "\n",
    "\n",
    "# Формируем обучающую и тестовую выборки\n",
    "X_train_emg, X_train_history, y_train = slicer_sequence(\n",
    "    data_train, label_train, fs, seq_len=5\n",
    ")\n",
    "X_test_emg, X_test_history, y_test = slicer_sequence(\n",
    "    data_test, label_test, fs, seq_len=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Код обучения модели с авто-регрессивной историей и teacher forcing\n",
    "# ----------------------------\n",
    "# Простейший MLP для объединения признаков из Тангенциального пространства и истории углов\n",
    "class DualInputMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ts_input_dim,\n",
    "        history_dim,\n",
    "        hidden_sizes=(400, 400),\n",
    "        output_dim=20,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super(DualInputMLP, self).__init__()\n",
    "        combined_dim = ts_input_dim + history_dim  # В нашем случае 21 + (seq_len * 20)\n",
    "        layers = []\n",
    "        current_dim = combined_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(current_dim, h))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            current_dim = h\n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, ts_features, history_features):\n",
    "        # Конкатенация признаков из тангенциального пространства и истории\n",
    "        x = torch.cat((ts_features, history_features), dim=1)\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RiemannMLPRegressorWithHistory(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=\"oas\",\n",
    "        metric=\"riemann\",\n",
    "        hidden_layer_sizes=(400, 400),\n",
    "        alpha=1e-5,\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        device=\"cpu\",\n",
    "        teacher_forcing_ratio=0.5,  # Вероятность использования ground truth для обновления истории\n",
    "        seq_len=5,  # Количество предыдущих шагов для истории углов\n",
    "    ):\n",
    "        self.estimator = estimator\n",
    "        self.metric = metric\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.device = device\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.cov_ = None\n",
    "        self.ts_ = None\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X_emg, X_history, y):\n",
    "        \"\"\"\n",
    "        Авто-регрессивное обучение с data augmentation в виде teacher forcing.\n",
    "        Поскольку мы хотим, чтобы модель училась работать с историей, составленной из собственных предсказаний,\n",
    "        в обучении мы игнорируем входной X_history и используем ground truth только для начальной инициализации истории.\n",
    "\n",
    "        Аргументы:\n",
    "          X_emg: массив с EMG-окнами, размер (N_samples, windowlen_samples, n_channels)\n",
    "          y: массив с целевыми углами, размер (N_samples, 20)\n",
    "        \"\"\"\n",
    "        # Вычисляем ковариационные матрицы и признаки в тангенциальном пространстве для всех примеров\n",
    "        self.cov_ = Covariances(estimator=self.estimator)\n",
    "        X_cov = self.cov_.fit_transform(X_emg)  # (N_samples, 6, 6)\n",
    "\n",
    "        self.ts_ = TangentSpace(metric=self.metric)\n",
    "        X_ts = self.ts_.fit_transform(X_cov)  # (N_samples, 21)\n",
    "        print(\"X_cov.shape:\", X_cov.shape)\n",
    "        print(\"X_ts.shape:\", X_ts.shape)\n",
    "\n",
    "        # Приводим данные к тензорам\n",
    "        X_ts_tensor = torch.tensor(X_ts, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Определяем размеры\n",
    "        ts_input_dim = X_ts_tensor.shape[1]  # 21\n",
    "        output_dim = y_tensor.shape[1]  # 20\n",
    "        history_dim = self.seq_len * output_dim  # например, 5*20\n",
    "\n",
    "        # Инициализируем модель\n",
    "        self.model_ = DualInputMLP(\n",
    "            ts_input_dim, history_dim, self.hidden_layer_sizes, output_dim\n",
    "        ).to(self.device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(\n",
    "            self.model_.parameters(), lr=0.001, weight_decay=self.alpha\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "\n",
    "        N = X_ts_tensor.shape[0]\n",
    "\n",
    "        # Обучаем модель проходом по всей последовательности в порядке времени.\n",
    "        for epoch in range(self.max_iter):\n",
    "            self.model_.train()\n",
    "            losses = []\n",
    "\n",
    "            # Инициализируем историю первыми seq_len ground truth значениями\n",
    "            # history: numpy-массив размера (seq_len, output_dim)\n",
    "            history = y_tensor[: self.seq_len].detach().cpu().numpy().copy()\n",
    "\n",
    "            # Проходим по последовательности начиная с индекса seq_len\n",
    "            for t in range(self.seq_len, N):\n",
    "                # Текущий TS-признак: форма (1, ts_input_dim)\n",
    "                ts_input = X_ts_tensor[t].unsqueeze(0)\n",
    "                # Формируем входную историю как вектор: (1, seq_len*output_dim)\n",
    "                history_input = torch.tensor(\n",
    "                    history.flatten()[np.newaxis, :], dtype=torch.float32\n",
    "                ).to(self.device)\n",
    "\n",
    "                pred = self.model_(ts_input, history_input)  # (1, output_dim)\n",
    "                loss = criterion(pred, y_tensor[t].unsqueeze(0))\n",
    "                losses.append(loss)\n",
    "\n",
    "                # Определяем, использовать ли ground truth (teacher forcing)\n",
    "                if np.random.rand() < self.teacher_forcing_ratio:\n",
    "                    new_val = y_tensor[t].detach().cpu().numpy()\n",
    "                else:\n",
    "                    new_val = pred.detach().cpu().numpy()[0]\n",
    "\n",
    "                # Обновляем историю: сдвигаем на один шаг и добавляем новое значение\n",
    "                history = np.roll(history, shift=-1, axis=0)\n",
    "                history[-1, :] = new_val\n",
    "\n",
    "            # Обновление параметров после прохождения всей последовательности\n",
    "            total_loss = sum(losses) / len(losses)\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1}/{self.max_iter}, Avg Loss: {total_loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def val(self, X_emg, X_history):\n",
    "        \"\"\"\n",
    "        Валидируем модель в режиме, когда история формируется из ground truth (без авто-регрессии).\n",
    "        \"\"\"\n",
    "        self.cov_ = Covariances(estimator=self.estimator)\n",
    "        X_cov = self.cov_.transform(X_emg)\n",
    "        self.ts_ = TangentSpace(metric=self.metric)\n",
    "        X_ts = self.ts_.transform(X_cov)\n",
    "\n",
    "        X_ts_tensor = torch.tensor(X_ts, dtype=torch.float32).to(self.device)\n",
    "        history_tensor = torch.tensor(X_history, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.model_(X_ts_tensor, history_tensor).cpu().numpy()\n",
    "        return preds\n",
    "\n",
    "    def predict(self, X_emg, initial_history=None):\n",
    "        \"\"\"\n",
    "        Авто-регрессионный инференс:\n",
    "          X_emg: массив с EMG-окнами, размер (N_samples, windowlen_samples, n_channels)\n",
    "          initial_history: начальное значение истории, размер (seq_len, output_dim).\n",
    "                           Если не передано, используется нулевая инициализация.\n",
    "        На каждом шаге используется история, составленная из предыдущих предсказаний.\n",
    "        \"\"\"\n",
    "        output_dim = self.model_.model[-1].out_features\n",
    "\n",
    "        if initial_history is None:\n",
    "            history_vec = np.zeros((self.seq_len, output_dim))\n",
    "        else:\n",
    "            history_vec = np.array(initial_history).reshape(self.seq_len, output_dim)\n",
    "\n",
    "        # Преобразуем всю последовательность EMG\n",
    "        X_cov = self.cov_.transform(X_emg)\n",
    "        X_ts = self.ts_.transform(X_cov)\n",
    "\n",
    "        predictions = []\n",
    "        num_samples = X_emg.shape[0]\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            ts_input = torch.tensor(X_ts[i : i + 1], dtype=torch.float32).to(\n",
    "                self.device\n",
    "            )\n",
    "            history_input = torch.tensor(\n",
    "                history_vec.flatten()[np.newaxis, :], dtype=torch.float32\n",
    "            ).to(self.device)\n",
    "            self.model_.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = self.model_(ts_input, history_input)\n",
    "            pred_np = pred.cpu().numpy()[0]\n",
    "            predictions.append(pred_np)\n",
    "            # Обновляем историю\n",
    "            history_vec = np.roll(history_vec, -1, axis=0)\n",
    "            history_vec[-1, :] = pred_np\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "# Создаем и обучаем модель\n",
    "model = RiemannMLPRegressorWithHistory(\n",
    "    estimator=\"oas\",\n",
    "    metric=\"riemann\",\n",
    "    hidden_layer_sizes=(400, 400),\n",
    "    max_iter=1000,\n",
    "    alpha=1e-5,\n",
    "    random_state=42,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    seq_len=5,\n",
    "    teacher_forcing_ratio=0.5,\n",
    ")\n",
    "\n",
    "# В обучении мы передаем только X_train_emg и y_train; X_train_history используется лишь для начальной инициализации (в методе val)\n",
    "model.fit(X_train_emg, X_train_history, y_train)\n",
    "y_model_pred = model.val(X_test_emg, X_test_history)\n",
    "\n",
    "results = {}\n",
    "correlations = []\n",
    "for i in range(y_test.shape[1]):\n",
    "    try:\n",
    "        corr = corrcoef(y_model_pred[:, i], y_test[:, i])\n",
    "        correlations.append(corr)\n",
    "    except:\n",
    "        correlations.append(0.0)\n",
    "\n",
    "results[\"model\"] = {\n",
    "    \"all_correlations\": correlations,\n",
    "    \"mean_correlation\": np.nanmean(correlations),\n",
    "    \"std_correlation\": np.nanstd(correlations),\n",
    "}\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(\n",
    "        f'  Mean correlation: {metrics[\"mean_correlation\"]:.3f} ± {metrics[\"std_correlation\"]:.3f}'\n",
    "    )\n",
    "    print(\n",
    "        f'  Individual correlations: {[f\"{c:.3f}\" for c in metrics[\"all_correlations\"]]}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5723a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model_pred = model.predict(X_test_emg)\n",
    "\n",
    "results = {}\n",
    "correlations = []\n",
    "for i in range(y_test.shape[1]):\n",
    "    try:\n",
    "        corr = corrcoef(y_model_pred[:, i], y_test[:, i])\n",
    "        correlations.append(corr)\n",
    "    except:\n",
    "        correlations.append(0.0)\n",
    "\n",
    "results[\"model\"] = {\n",
    "    \"all_correlations\": correlations,\n",
    "    \"mean_correlation\": np.nanmean(correlations),\n",
    "    \"std_correlation\": np.nanstd(correlations),\n",
    "}\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(\n",
    "        f'  Mean correlation: {metrics[\"mean_correlation\"]:.3f} ± {metrics[\"std_correlation\"]:.3f}'\n",
    "    )\n",
    "    print(\n",
    "        f'  Individual correlations: {[f\"{c:.3f}\" for c in metrics[\"all_correlations\"]]}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.plot(y_test[:, i])\n",
    "    plt.plot(y_model_pred[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ccbfa",
   "metadata": {},
   "source": [
    "## Conv Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "subj_path = \"..\\\\data\\\\Fedor\\\\Raw\\\\preproc_angles\\\\1\\\\\"\n",
    "fs = 500\n",
    "\n",
    "\n",
    "def corrcoef(x, y):\n",
    "    if np.std(x) == 0 or np.std(y) == 0:\n",
    "        return 0.0\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "\n",
    "def train_test_split(data, N_parts, num_of_part):\n",
    "    N_samples = len(data)\n",
    "\n",
    "    l_idx = int((N_samples * num_of_part) / N_parts)\n",
    "    h_idx = int((N_samples * (num_of_part + 1)) / N_parts)\n",
    "\n",
    "    data_train = np.concatenate([data[:l_idx, :], data[h_idx:,]], axis=0)\n",
    "    data_test = data[l_idx:h_idx, :]\n",
    "\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "N_files = 4\n",
    "data_list_train = list()\n",
    "data_list_test = list()\n",
    "label_list_train = list()\n",
    "label_list_test = list()\n",
    "\n",
    "N_parts = 10\n",
    "num_of_part = 9\n",
    "\n",
    "for i in range(N_files):\n",
    "\n",
    "    arr = np.load(subj_path + \"000\" + str(i) + \".npz\")\n",
    "\n",
    "    std_coef = arr[\"std_coef\"]\n",
    "    data = arr[\"data_myo\"]\n",
    "    label = arr[\"data_angles\"]\n",
    "\n",
    "    data_train, data_test = train_test_split(data, N_parts, num_of_part)\n",
    "    label_train, label_test = train_test_split(label, N_parts, num_of_part)\n",
    "\n",
    "    data_list_train.append(data_train)\n",
    "    data_list_test.append(data_test)\n",
    "\n",
    "    label_list_train.append(label_train)\n",
    "    label_list_test.append(label_test)\n",
    "\n",
    "\n",
    "data_train = np.concatenate(data_list_train, axis=0)\n",
    "data_test = np.concatenate(data_list_test, axis=0)\n",
    "\n",
    "\n",
    "label_train = np.concatenate(label_list_train, axis=0)\n",
    "label_test = np.concatenate(label_list_test, axis=0)\n",
    "\n",
    "\n",
    "def slicer(data, label, fs, windowlen=500, timestep=100):\n",
    "    data_len = len(data)\n",
    "    timestep_samples = int((timestep * fs) / 1000)\n",
    "    windowlen_samples = int((windowlen * fs) / 1000)\n",
    "    start_idc = np.arange(0, data_len - windowlen_samples, timestep_samples)[:, None]\n",
    "    window_idc = np.arange(0, windowlen_samples)[None, :]\n",
    "    slice_idc = start_idc + window_idc\n",
    "    slice_data = data[slice_idc].transpose(0, 2, 1)\n",
    "    slice_label = label[start_idc[:, 0] + windowlen_samples]\n",
    "    return slice_data, slice_label\n",
    "\n",
    "\n",
    "X_train, y_train = slicer(data_train, label_train, fs)\n",
    "X_test, y_test = slicer(data_test, label_test, fs)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx].to(torch.float32), self.y[idx].to(torch.float32)\n",
    "\n",
    "\n",
    "train_dataset = EMGDataset(X_train, y_train)\n",
    "val_dataset = EMGDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_joints = 20\n",
    "N_EMG = 6\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, downsample=None\n",
    "    ):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=1,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, kernel_size=3, num_blocks=25, block_expansion=N_joints, strides=[2, 2, 2]\n",
    "    ):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(N_joints, N_EMG), requires_grad=True)\n",
    "\n",
    "        # Изменили число входных каналов с 1 на N_joints,\n",
    "        # так как после матричного умножения размер тензора: [batch_size, N_joints, window_size]\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            N_joints, block_expansion, kernel_size, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(block_expansion)\n",
    "\n",
    "        # Создаем и объединяем остаточные блоки\n",
    "        self.res_layers = self._make_layer(\n",
    "            ResidualBlock, block_expansion, num_blocks, kernel_size\n",
    "        )\n",
    "\n",
    "        self.downsample_blocks = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    block_expansion, block_expansion, kernel_size=stride, stride=stride\n",
    "                )\n",
    "                for stride in strides\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            block_expansion, N_joints, kernel_size, padding=1, bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(N_joints)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, kernel_size):\n",
    "        layers = []\n",
    "        in_channels = out_channels\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(block(in_channels, out_channels, kernel_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: [batch_size(64), N_EMG(8), window_size(512)]\n",
    "        # После матричного умножения -> [batch_size, N_joints, window_size]\n",
    "        x = torch.abs(torch.matmul(self.weight, x))\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        for down_block in self.downsample_blocks:\n",
    "            x = down_block(x)\n",
    "\n",
    "        x = self.res_layers(x)\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = x[..., -1:].squeeze(-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Гиперпараметры\n",
    "epochs = 1_500\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = SimpleNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=100, verbose=True, min_lr=1e-6)\n",
    "\n",
    "\n",
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            # print(f\"ouputs: {outputs.shape}\", f\"targets: {targets.shape}\")\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Обновление шедулера\n",
    "        scheduler.step()\n",
    "        # scheduler.step(val_loss)\n",
    "\n",
    "        # Логирование\n",
    "        if epoch % 100 == 0:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            # current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "subj_path = \"..\\\\data\\\\Fedor\\\\Raw\\\\preproc_angles\\\\1\\\\\"\n",
    "fs = 500\n",
    "\n",
    "\n",
    "def corrcoef(x, y):\n",
    "    if np.std(x) == 0 or np.std(y) == 0:\n",
    "        return 0.0\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "\n",
    "def train_test_split(data, N_parts, num_of_part):\n",
    "    N_samples = len(data)\n",
    "\n",
    "    l_idx = int((N_samples * num_of_part) / N_parts)\n",
    "    h_idx = int((N_samples * (num_of_part + 1)) / N_parts)\n",
    "\n",
    "    data_train = np.concatenate([data[:l_idx, :], data[h_idx:,]], axis=0)\n",
    "    data_test = data[l_idx:h_idx, :]\n",
    "\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "N_files = 4\n",
    "data_list_train = list()\n",
    "data_list_test = list()\n",
    "label_list_train = list()\n",
    "label_list_test = list()\n",
    "\n",
    "N_parts = 10\n",
    "num_of_part = 9\n",
    "\n",
    "for i in range(N_files):\n",
    "\n",
    "    arr = np.load(subj_path + \"000\" + str(i) + \".npz\")\n",
    "\n",
    "    std_coef = arr[\"std_coef\"]\n",
    "    data = arr[\"data_myo\"]\n",
    "    label = arr[\"data_angles\"]\n",
    "\n",
    "    data_train, data_test = train_test_split(data, N_parts, num_of_part)\n",
    "    label_train, label_test = train_test_split(label, N_parts, num_of_part)\n",
    "\n",
    "    data_list_train.append(data_train)\n",
    "    data_list_test.append(data_test)\n",
    "\n",
    "    label_list_train.append(label_train)\n",
    "    label_list_test.append(label_test)\n",
    "\n",
    "\n",
    "data_train = np.concatenate(data_list_train, axis=0)\n",
    "data_test = np.concatenate(data_list_test, axis=0)\n",
    "\n",
    "\n",
    "label_train = np.concatenate(label_list_train, axis=0)\n",
    "label_test = np.concatenate(label_list_test, axis=0)\n",
    "\n",
    "\n",
    "def slicer(data, label, fs, windowlen=500, timestep=100):\n",
    "    data_len = len(data)\n",
    "    timestep_samples = int((timestep * fs) / 1000)\n",
    "    windowlen_samples = int((windowlen * fs) / 1000)\n",
    "    start_idc = np.arange(0, data_len - windowlen_samples, timestep_samples)[:, None]\n",
    "    window_idc = np.arange(0, windowlen_samples)[None, :]\n",
    "    slice_idc = start_idc + window_idc\n",
    "    slice_data = data[slice_idc].transpose(0, 2, 1)\n",
    "    slice_label = label[start_idc[:, 0] + windowlen_samples]\n",
    "    return slice_data, slice_label\n",
    "\n",
    "\n",
    "X_train, y_train = slicer(data_train, label_train, fs)\n",
    "X_test, y_test = slicer(data_test, label_test, fs)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class EMGDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx].to(torch.float32), self.y[idx].to(torch.float32)\n",
    "\n",
    "\n",
    "train_dataset = EMGDataset(X_train, y_train)\n",
    "val_dataset = EMGDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "N_joints = 20\n",
    "N_EMG = 6\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, downsample=None\n",
    "    ):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=1,\n",
    "            padding=(kernel_size - 1) // 2,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=3,\n",
    "        num_blocks=25,\n",
    "        block_expansion=N_joints,\n",
    "        strides=[2, 2, 2, 2, 2, 2, 2],\n",
    "    ):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(N_joints, N_EMG), requires_grad=True)\n",
    "\n",
    "        # Изменили число входных каналов с 1 на N_joints,\n",
    "        # так как после матричного умножения размер тензора: [batch_size, N_joints, window_size]\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            N_joints, block_expansion, kernel_size, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(block_expansion)\n",
    "\n",
    "        # Создаем и объединяем остаточные блоки\n",
    "        self.res_layers = self._make_layer(\n",
    "            ResidualBlock, block_expansion, num_blocks, kernel_size\n",
    "        )\n",
    "\n",
    "        self.downsample_blocks = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    block_expansion, block_expansion, kernel_size=stride, stride=stride\n",
    "                )\n",
    "                for stride in strides\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            block_expansion, N_joints, kernel_size, padding=1, bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(N_joints)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, kernel_size):\n",
    "        layers = []\n",
    "        in_channels = out_channels\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(block(in_channels, out_channels, kernel_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: [batch_size(64), N_EMG(8), window_size(512)]\n",
    "        # После матричного умножения -> [batch_size, N_joints, window_size]\n",
    "        x = torch.abs(torch.matmul(self.weight, x))\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        for down_block in self.downsample_blocks:\n",
    "            x = down_block(x)\n",
    "\n",
    "        x = self.res_layers(x)\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = x.squeeze(-1)\n",
    "        # print(\"x.shape:\", x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Гиперпараметры\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = SimpleNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=100, verbose=True, min_lr=1e-6)\n",
    "\n",
    "\n",
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            # print(f\"ouputs: {outputs.shape}\", f\"targets: {targets.shape}\")\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Обновление шедулера\n",
    "        scheduler.step()\n",
    "        # scheduler.step(val_loss)\n",
    "\n",
    "        # Логирование\n",
    "        if epoch % 100 == 0:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            # current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_preds = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Получение предсказаний от модели\n",
    "        outputs = model(inputs).cpu().detach().numpy()  # Преобразование в numpy-массив\n",
    "\n",
    "        # Сохранение предсказанных и целевых значений\n",
    "        y_preds.append(outputs)\n",
    "        y_true.append(targets.cpu().numpy())\n",
    "\n",
    "# Объединение всех предсказаний и таргетов в один массив\n",
    "y_preds = np.concatenate(y_preds, axis=0)\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "# Проверка совпадения размеров объединенных массивов\n",
    "assert (\n",
    "    y_preds.shape == y_true.shape\n",
    "), \"Размерности предсказаний и целевых значений должны совпадать\"\n",
    "\n",
    "# Вычисление корреляции между предсказанными значениями и истинными значениями\n",
    "correlation = np.corrcoef(y_preds, y_true)[0, 1]\n",
    "\n",
    "print(f\"Корреляция между предсказаниями и целевыми значениями: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_true.shape[1]):\n",
    "    plt.figure()\n",
    "    plt.plot(y_true[:, i])\n",
    "    plt.plot(y_preds[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EMGHandPoseCNN(nn.Module):\n",
    "    def __init__(self, input_channels=6, seq_length=256, output_size=20):\n",
    "        super(EMGHandPoseCNN, self).__init__()\n",
    "\n",
    "        # Блок 1: Временные особенности\n",
    "        self.temporal_block = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=15, padding=7),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.Conv1d(64, 128, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.Conv1d(128, 256, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "        )\n",
    "\n",
    "        # Блок 2: Пространственные зависимости между каналами\n",
    "        self.spatial_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(6, 1), padding=0),  # Объединение каналов\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "            nn.AdaptiveAvgPool2d((1, None)),\n",
    "        )\n",
    "\n",
    "        # Блок 3: Глубокие временные особенности\n",
    "        self.deep_temporal = nn.Sequential(\n",
    "            nn.Conv1d(32, 512, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Conv1d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.AdaptiveMaxPool1d(8),\n",
    "        )\n",
    "\n",
    "        # Регрессионный блок\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(512 * 8, 1024),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, output_size),\n",
    "        )\n",
    "\n",
    "        # Инициализация весов\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"gelu\")\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Вход: (batch_size, 6, 256)\n",
    "\n",
    "        # Временные особенности\n",
    "        x = self.temporal_block(x)  # (batch_size, 256, 16)\n",
    "\n",
    "        # Пространственная обработка\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, 256, 16)\n",
    "        x = self.spatial_block(x)  # (batch_size, 32, 1, 16)\n",
    "        x = x.squeeze(2)  # (batch_size, 32, 16)\n",
    "\n",
    "        # Глубокие временные зависимости\n",
    "        x = self.deep_temporal(x)  # (batch_size, 512, 8)\n",
    "\n",
    "        # Регрессия\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.regression(x)\n",
    "\n",
    "        return x  # (batch_size, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
