{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "456329b6",
   "metadata": {},
   "source": [
    "# Inference on validation data\n",
    "\n",
    "This notebook provides how to make prediction and visualize movement prediction to mp4 files.\n",
    "\n",
    "Also you can stack prediction horizontally for comparing difference between prediction and real movements \n",
    "\n",
    "Default parameters: \n",
    "\n",
    "- FPS = 200\n",
    "- NEW_FPS = 25\n",
    "- WINDOW_SIZE = 512\n",
    "- STRIDE = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba223b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb, sys, os  \n",
    "import numpy as np\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "from utils import data_utils, losses\n",
    "from utils.hand_visualize import Hand, save_animation_mp4, visualize_and_save_anim, merge_two_videos, visualize_and_save_anim_gifs #, merge_two_videos_vertically\n",
    "from utils.inference_utils import make_inference, calculcate_latency, get_angle_degree\n",
    "from utils.quats_and_angles import get_quats, get_angles\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "from models import HVATNet_v2, HVATNet_v3, HVATNet_v3_FineTune\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "897d85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "def upload_weights_pl(model, path, pt):\n",
    "    class Lit_Wrapper(pl.LightningModule):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "        def forward(self, x):\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "    \n",
    "    if pt == 'ckpt':\n",
    "        ckpt = torch.load(path, map_location=torch.device('cpu'))\n",
    "        model_pl = Lit_Wrapper(model)\n",
    "        model_pl.load_state_dict(ckpt['state_dict'])\n",
    "    else:\n",
    "        try:\n",
    "            ckpt = torch.load(path, map_location=torch.device('cpu'))\n",
    "            model_pl = Lit_Wrapper(model)\n",
    "            model_pl.load_state_dict(ckpt)\n",
    "        except:\n",
    "            ckpt = torch.load(path, map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(ckpt)\n",
    "            return model\n",
    "    \n",
    "    return model_pl.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7f3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    WANDB_NOTES = 'HVATNet v3 FT train on all new data + no augs'\n",
    "\n",
    "    datasets = ['../../data/general_set']\n",
    "    # datasets = ['../../data/processed/dataset_v2_blocks']\n",
    "    \n",
    "    # hand_type = ['right']\n",
    "    # human_type = ['health']\n",
    "    hand_type = ['left']\n",
    "    human_type = ['amputant']\n",
    "\n",
    "    test_dataset_list = ['5']\n",
    "\n",
    "    use_preproc_data = True # use preproc data (faster preparation  \n",
    "    use_angles = True # use angels as target.\n",
    "    \n",
    "    original_fps = 250  # TODO describtion\n",
    "    delay_ms = 0  # Shift vr vs EMG parameter. Do not work with preproc data. Fix it!!\n",
    "    start_crop_ms = 0  # bad values in the beginning of recordign in ms to delete.\n",
    "    window_size = 256\n",
    "    down_sample_target = 8 # None\n",
    "\n",
    "    max_epochs = 3000\n",
    "    samples_per_epoch = 5000*256\n",
    "    train_bs = 2048\n",
    "    val_bs = 2048\n",
    "    device = [2]\n",
    "    optimizer_params = dict(lr=1e-4,\n",
    "                            wd=0)\n",
    "config = TrainConfig()\n",
    "\n",
    "train_paths, val_paths = data_utils.get_train_val_pathes(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0227cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = Path('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train')\n",
    "files = list(rootdir.glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "983d16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9387c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = Path('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Validation')\n",
    "files = list(rootdir.glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04523a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Validation/2_3'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Validation/4_4'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Validation/4_5'),\n",
       " WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Validation/m3_5')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01683e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_paths = [files[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f238bd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Validation/2_3')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81950c",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e40e302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of moves: 1 | Dataset: GeneralTraining\n"
     ]
    }
   ],
   "source": [
    "# Init train and val dataset and dataloaders\n",
    "val_datasets = []\n",
    "for val_folder in val_paths:\n",
    "    val_dataset = data_utils.create_dataset(data_folder=val_folder,\n",
    "                                            original_fps=config.original_fps,\n",
    "                                            delay_ms=config.delay_ms,\n",
    "                                            start_crop_ms=config.start_crop_ms,\n",
    "                                            window_size=config.window_size,\n",
    "\n",
    "                                            down_sample_target = config.down_sample_target, \n",
    "                                            use_preproc_data=config.use_preproc_data, \n",
    "                                            use_angles=config.use_angles, \n",
    "\n",
    "                                            random_sampling = False,\n",
    "                                            samples_per_epoch = None, \n",
    "                                            transform = None)\n",
    "    \n",
    "    val_datasets.append(val_dataset)\n",
    "\n",
    "val_dataset = torch.utils.data.ConcatDataset(val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d522b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.ConcatDataset(val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0e21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'HVATNet_v3_small_unet_FT_angle'\n",
    "DATA_TYPE = 'nast_best_model'\n",
    "ANGLE_DEGREE = '13'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4e419",
   "metadata": {},
   "source": [
    "## Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc5da5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  4210788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root = 'C:/Users/vlvdi/Desktop/EMG/alvi/weights/'\n",
    "\n",
    "CKPT_PATH = root + 'pretrained_all_nast_6.016.pt'\n",
    "#CKPT_PATH = artifact_dir / 'epoch=23_val_angle_degree=8.682.ckpt'\n",
    "\n",
    "params =dict(n_electrodes=8, n_channels_out=20,\n",
    "                        n_res_blocks=3, n_blocks_per_layer=3,\n",
    "                        n_filters=128, kernel_size=3,\n",
    "                        strides=(2, 2, 2),\n",
    "                        dilation=2, use_angles = False)\n",
    "\n",
    "model = HVATNet_v3_FineTune.HVATNetv3(**params)\n",
    "model.use_angles = True\n",
    "\n",
    "model = upload_weights_pl(model, CKPT_PATH, CKPT_PATH[-4:])\n",
    "\n",
    "#model.load_state_dict(torch.load(CKPT_PATH, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fada8d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7824,  0.1518,  0.2810,  0.1888,  0.1750,  0.2631,  0.3175,  0.0570],\n",
       "        [ 0.1442,  0.7647,  0.0708,  0.1398,  0.1881,  0.2690,  0.3261,  0.2829],\n",
       "        [ 0.2185,  0.0531,  0.7363,  0.0091,  0.1638,  0.2452,  0.3222,  0.2985],\n",
       "        [ 0.2124,  0.1643,  0.0782,  0.7879, -0.0666,  0.1827,  0.3160,  0.2695],\n",
       "        [ 0.1967,  0.2016,  0.2038,  0.0171,  0.7882,  0.0988,  0.2837,  0.2546],\n",
       "        [ 0.2245,  0.2594,  0.2554,  0.1814,  0.0082,  0.7817,  0.1932,  0.2614],\n",
       "        [ 0.2868,  0.3239,  0.3231,  0.2942,  0.2764, -0.0134,  0.6971,  0.0507],\n",
       "        [ 0.0888,  0.2627,  0.2954,  0.2264,  0.1954,  0.2002,  0.2157,  0.7339]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['tune_module.spatial_weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36e8f9",
   "metadata": {},
   "source": [
    "## Apply inference for each moves.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c32f8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'C:/Users/vlvdi/Desktop/EMG/alvi/model_nast_simple.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dfddadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = torch.utils.data.ConcatDataset(val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "098a09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf8a8c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22240, 8)\n",
      "(2752, 16, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2752, 20)\n",
      "Size of targets (2752, 20) || Size of preds (2752, 20) \n",
      "Window size 256 || Stride 256\n",
      "Angle degree:  8.889 0.0\n",
      "Cosine similarity:  0.525 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "FPS = 25\n",
    "NEW_FPS = 25\n",
    "DRAW_EVERY = 200 // NEW_FPS\n",
    "\n",
    "WINDOW_SIZE = 256\n",
    "STRIDE = 256\n",
    "\n",
    "corr_list = []\n",
    "angle_degree_list = []\n",
    "preds_per_person = []\n",
    "targets_per_person = []\n",
    "abs_diff_per_person = []\n",
    "angle_diff_per_person = []\n",
    "\n",
    "for val_dataset_sample in val_dataset.datasets:\n",
    "    \n",
    "    all_move_targets = []\n",
    "    all_move_preds = []\n",
    "    all_myo = []\n",
    "    \n",
    "    for move_data in tqdm(val_dataset_sample.exps_data): \n",
    "        data_myo, data_vr = move_data['data_myo'], move_data['data_angles']\n",
    "        print(data_myo.shape)\n",
    "        preds, targets = make_inference(data_myo, data_vr, model,\n",
    "                                        window_size=WINDOW_SIZE, \n",
    "                                        stride=STRIDE, \n",
    "                                        device='cpu', \n",
    "                                        return_angles=True)\n",
    "        print(preds.shape)\n",
    "        preds = get_angles(preds)\n",
    "        print(preds.shape)\n",
    "        if config.down_sample_target is not None:\n",
    "            targets = targets[::config.down_sample_target]\n",
    "\n",
    "        all_move_targets.append(targets)\n",
    "        all_move_preds.append(preds)\n",
    "        all_myo.append(data_myo)\n",
    "\n",
    "    targets = np.concatenate(all_move_targets, axis=0)\n",
    "    preds = np.concatenate(all_move_preds, axis=0)\n",
    "    all_myo = np.concatenate(all_myo, axis=0)\n",
    "\n",
    "    preds_per_person.append(all_move_preds)\n",
    "    targets_per_person.append(all_move_targets)\n",
    "    diff = np.abs(targets - preds)\n",
    "    abs_diff_per_person.append(diff)\n",
    "    mean_diff_angle_per_joint = np.rad2deg(np.mean(diff, axis = 1))\n",
    "    angle_diff_per_person.append(mean_diff_angle_per_joint)\n",
    "    \n",
    "    # our metrics:\n",
    "    dif = np.mean(diff)\n",
    "    angle_degree = np.round(np.rad2deg(dif), 3)\n",
    "\n",
    "    corr_coef = torch.mean(F.cosine_similarity(torch.from_numpy(targets), torch.from_numpy(preds), dim=0, eps=1e-8))\n",
    "    corr_coef = np.round(corr_coef.item(), 3)\n",
    "    \n",
    "    corr_list.append(corr_coef)\n",
    "    angle_degree_list.append(angle_degree)\n",
    "    \n",
    "\n",
    "# angle_degree = np.round(F.preds, targets), 3)\n",
    "\n",
    "print(f\"Size of targets {targets.shape} || Size of preds {preds.shape} \") \n",
    "print(f\"Window size {WINDOW_SIZE} || Stride {STRIDE}\")\n",
    "print('Angle degree: ', np.mean(angle_degree_list), np.std(angle_degree_list))\n",
    "print('Cosine similarity: ', np.mean(corr_list), np.std(corr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e09dfff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.081395348837209"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(22240, 8)\n",
    "(2752, 16, 4)\n",
    "\n",
    "(2752, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "26b7ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_quats = get_quats(preds)\n",
    "tar_quats = get_quats(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6eaab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_FPS = 20\n",
    "DRAW_EVERY = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e18f067c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video mar_preds_simple_tese.mp4 completed\n"
     ]
    }
   ],
   "source": [
    "visualize_and_save_anim(data=pred_quats[::DRAW_EVERY],\n",
    "                        path=f'mar_preds_simple_tese.mp4', \n",
    "                        fps=NEW_FPS)\n",
    "\n",
    "# visualize_and_save_anim(data=tar_quats[::DRAW_EVERY],\n",
    "#                         path='targ_simple_thumb.mp4', \n",
    "#                         fps=NEW_FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57163a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred_quats[:, [1, 2, 3, 4, 5, 6, 7, 10, 13, 14, 15], 0], color='lightcoral', alpha=0.75)\n",
    "plt.plot(tar_quats[:, [1, 2, 3, 4, 5, 6, 7, 10, 13, 14, 15], 0], color='skyblue', alpha=0.75)\n",
    "plt.xlabel('Time, samples', fontsize=12)\n",
    "plt.ylabel('Scaled amplitude', fontsize=12)\n",
    "plt.legend(['Prediction', 'Ground Truth'], fontsize=12)\n",
    "\n",
    "ax = plt.gca()\n",
    "leg = ax.get_legend()\n",
    "leg.legendHandles[0].set_color('lightcoral')\n",
    "leg.legendHandles[1].set_color('skyblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_path = 'C:/Users/vlvdi/Desktop/EMG/MYO_DATA/Marusya/predictions/pretrained_arrays/'\n",
    "# np.save(init_path + f'preds_simple_{finger}.npy', pred_quats)\n",
    "# np.save(init_path + f'targ_simple_{finger}.npy', tar_quats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a551a8ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_quats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# render and saving as gif files\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m visualize_and_save_anim_gifs(data\u001b[38;5;241m=\u001b[39m\u001b[43mpred_quats\u001b[49m[::DRAW_EVERY],\n\u001b[0;32m      3\u001b[0m                         path\u001b[38;5;241m=\u001b[39mPath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/vlvdi/Desktop/EMG/alvi/index_latest.gif\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[0;32m      4\u001b[0m                         fps\u001b[38;5;241m=\u001b[39mNEW_FPS)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_quats' is not defined"
     ]
    }
   ],
   "source": [
    "# render and saving as gif files\n",
    "visualize_and_save_anim_gifs(data=pred_quats[::DRAW_EVERY],\n",
    "                        path=Path('C:/Users/vlvdi/Desktop/EMG/alvi/index_latest.gif'), \n",
    "                        fps=NEW_FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26644f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
