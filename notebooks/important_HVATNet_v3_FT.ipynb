{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82faea8f-ce77-4f54-8034-58a0bd23c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "from pytorch_model_summary import summary\n",
    "from natsort import natsorted\n",
    "\n",
    "from utils import data_utils\n",
    "from utils import common, losses, hand_visualize\n",
    "from models import HVATNet_v3, HVATNet_v3_FineTune\n",
    "import audiomentations as A\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371618af-fc9d-4b8f-8f12-97438ad7bf61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09754d6d-3cb7-4a05-881d-08eec8ca3dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed all random things with randow seed 42\n"
     ]
    }
   ],
   "source": [
    "common.make_it_reproducible()\n",
    "\n",
    "\n",
    "class TrainConfig:\n",
    "    WANDB_NOTES = \"HVATNet v3 FT: large bs + augs + wd\"\n",
    "\n",
    "    # datasets = [\"../../data/processed/dataset_v1_big\"]\n",
    "\n",
    "    hand_type = [\"left\"]  # ['left', 'right']\n",
    "    human_type = [\"amputant\"]  # ['health', 'amputant']\n",
    "\n",
    "    use_preproc_data = True  # use preproc data (faster preparation\n",
    "    use_angles = True  # use angeles as target.\n",
    "\n",
    "    original_fps = 250  # TODO describtion\n",
    "    delay_ms = 0  # Shift vr vs EMG parameter. Do not work with preproc data. Fix it!!\n",
    "    start_crop_ms = 0  # bad values in the beginning of recordign in ms to delete.\n",
    "    window_size = 256\n",
    "    down_sample_target = 8  # None\n",
    "\n",
    "    max_epochs = 3000\n",
    "    samples_per_epoch = 1000 * 256\n",
    "    train_bs = 512\n",
    "    # train_bs = 256\n",
    "    val_bs = 512\n",
    "    # val_bs = 256\n",
    "    device = [0]  # [0]\n",
    "    optimizer_params = dict(lr=1e-4, wd=1e-6)\n",
    "\n",
    "\n",
    "config = TrainConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb7c4597",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = Path(\"C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Train\")\n",
    "files = list(rootdir.glob(\"*\"))\n",
    "train_paths = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49922389",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = Path(\n",
    "    \"C:/Users/vlvdi/Desktop/EMG/EMG_TRAINING/Nastya/GeneralTraining/Validation\"\n",
    ")\n",
    "\n",
    "\n",
    "files = list(rootdir.glob(\"*\"))\n",
    "\n",
    "\n",
    "val_paths = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8f401-b683-422b-8dde-4972b4c80f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # augmentations\n",
    "# transform = A.Compose([\n",
    "# A.AddGaussianNoise(min_amplitude=0.01, max_amplitude=0.1, p=0.3),\n",
    "# data_utils.SpatialRotation(min_angle=1, max_angle=10, p=0.5)\n",
    "# ])\n",
    "transform = None\n",
    "\n",
    "# Init train and val dataset and dataloaders\n",
    "train_datasets = []\n",
    "for train_folder in train_paths:\n",
    "    train_dataset = data_utils.create_dataset(\n",
    "        data_folder=train_folder,\n",
    "        original_fps=config.original_fps,\n",
    "        delay_ms=config.delay_ms,\n",
    "        start_crop_ms=config.start_crop_ms,\n",
    "        window_size=config.window_size,\n",
    "        down_sample_target=config.down_sample_target,\n",
    "        use_preproc_data=config.use_preproc_data,\n",
    "        use_angles=config.use_angles,\n",
    "        random_sampling=True,\n",
    "        samples_per_epoch=config.samples_per_epoch,  # // len(train_paths),\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"WWWWW: Problem with dataset\")\n",
    "        break\n",
    "    train_datasets.append(train_dataset)\n",
    "\n",
    "val_datasets = []\n",
    "for val_folder in val_paths:\n",
    "    val_dataset = data_utils.create_dataset(\n",
    "        data_folder=val_folder,\n",
    "        original_fps=config.original_fps,\n",
    "        delay_ms=config.delay_ms,\n",
    "        start_crop_ms=config.start_crop_ms,\n",
    "        window_size=config.window_size,\n",
    "        down_sample_target=config.down_sample_target,\n",
    "        use_preproc_data=config.use_preproc_data,\n",
    "        use_angles=config.use_angles,\n",
    "        random_sampling=False,\n",
    "        samples_per_epoch=None,\n",
    "        transform=None,\n",
    "    )\n",
    "\n",
    "    val_datasets.append(val_dataset)\n",
    "\n",
    "train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
    "val_dataset = torch.utils.data.ConcatDataset(val_datasets)\n",
    "print(f\"Size of the train dataset {len(train_dataset)}\")\n",
    "for kek in val_datasets:\n",
    "    print(f\"Size of the val dataset {len(kek)}\")\n",
    "\n",
    "print(\"-\")\n",
    "print(\n",
    "    f\"Size of the train dataset {len(train_dataset)} || Size of the val dataset {len(val_dataset)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Size of the input {train_dataset[0][0].shape} || Size of the output {train_dataset[0][1].shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd1870-c38d-44ed-8906-b4132dfeb835",
   "metadata": {},
   "source": [
    "# Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff294df6-750d-45e8-aab3-dbe84c14d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitHVATNet_v3(pl.LightningModule):\n",
    "    def __init__(self, model, lr, wd):\n",
    "        \"\"\"\n",
    "        Wrapper of model with loss function calculatino and initing optimizer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.lr, weight_decay=self.wd\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def step(self, train_batch):\n",
    "        x, y = train_batch\n",
    "        full_size_pred = self.model(x)\n",
    "        mae_loss = self.mae_loss(full_size_pred, y)\n",
    "        #         mse_loss = torch.sqrt(self.mse_loss(full_size_pred, y))\n",
    "\n",
    "        cosine_sim = torch.mean(\n",
    "            F.cosine_similarity(full_size_pred, y, dim=-1, eps=1e-8)\n",
    "        )\n",
    "\n",
    "        loss_dict = {\n",
    "            \"total_loss\": mae_loss,\n",
    "            \"angle_degree\": mae_loss * 180 / 3.14,\n",
    "            \"cosine_sim\": cosine_sim,\n",
    "        }\n",
    "        return loss_dict\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "\n",
    "        loss_dict = self.step(train_batch)\n",
    "\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"train_\" + str(k), v, on_step=True)\n",
    "        print(\".\", end=\"\")\n",
    "        return loss_dict[\"total_loss\"]\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        if trainer.global_step == 0:\n",
    "            wandb.define_metric(\"val_angle_degree\", summary=\"min\")\n",
    "\n",
    "        loss_dict = self.step(val_batch)\n",
    "\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"val_\" + str(k), v, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.validation_step_outputs.append(loss_dict[\"angle_degree\"])\n",
    "\n",
    "        return loss_dict[\"angle_degree\"]\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_current_loss = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.val_current_loss = val_current_loss\n",
    "\n",
    "        self.validation_step_outputs.clear()\n",
    "        print(\n",
    "            f\"current step {self.current_epoch} val_current_loss {self.val_current_loss}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a954acee-d1e6-4423-a576-8d6d0316686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovementsWandb(Callback):\n",
    "    def __init__(self, pred_fps=25):\n",
    "        self.pred_fps = pred_fps\n",
    "        self.best_val_loss = 10000000\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "\n",
    "        ### do not calculate epoch in sanity check.\n",
    "        if trainer.state.stage == \"sanity_check\":\n",
    "            return\n",
    "\n",
    "        # check better or not\n",
    "        if pl_module.val_current_loss < self.best_val_loss:\n",
    "\n",
    "            print(\"new best val score\", pl_module.val_current_loss)\n",
    "\n",
    "            self.best_val_loss = pl_module.val_current_loss\n",
    "\n",
    "            # VLAD: commented this\n",
    "\n",
    "\n",
    "#             hand_visualize.visualize_val_moves(model = pl_module.model,\n",
    "#                                                val_exps_data = trainer.val_dataloaders.dataset.datasets[0].exps_data,\n",
    "#                                                epoch = pl_module.current_epoch,\n",
    "#                                                device = pl_module.device,\n",
    "#                                                pred_fps = self.pred_fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d59243-afc6-49f1-876d-3e0b2a4c9fe7",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac825d5d-19e7-43af-a3bb-5162b0d3cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRETRAIN_PATH = Path('../weights/hvatnet_v3_angles_full_data.pt')\n",
    "\n",
    "\n",
    "hvatnet_v3_params = dict(\n",
    "    n_electrodes=8,\n",
    "    n_channels_out=20,\n",
    "    n_res_blocks=3,\n",
    "    n_blocks_per_layer=3,\n",
    "    n_filters=128,\n",
    "    kernel_size=3,\n",
    "    strides=(2, 2, 2),\n",
    "    dilation=2,\n",
    "    use_angles=config.use_angles,\n",
    ")\n",
    "\n",
    "model = HVATNet_v3_FineTune.HVATNetv3(**hvatnet_v3_params)\n",
    "# model.load_state_dict(torch.load(PRETRAIN_PATH, map_location='cpu'))\n",
    "\n",
    "# model = torch.compile()\n",
    "model_pl = LitHVATNet_v3(model, **config.optimizer_params)\n",
    "# model_pl = torch.compile(model_pl)\n",
    "\n",
    "x = torch.zeros([2, 8, 256])\n",
    "y = model(x)\n",
    "print(summary(model.to(\"cpu\"), x, show_input=True))\n",
    "print(\"Input shape: \", x.shape)\n",
    "print(\"Output shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a264a890-e05a-400a-8c19-e56d07bf01b9",
   "metadata": {},
   "source": [
    "### Start to train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45aeec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=config.train_bs, shuffle=True, num_workers=3\n",
    ")\n",
    "\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=config.train_bs, shuffle=False, num_workers=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27004b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = (\n",
    "    r\"D:\\study\\emg\\machine-learning\\notebooks\\important_HVATNet_v3_FT.ipynb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "438d375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"myo_prost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e32e91d-83d0-4888-a599-3fbfbe82864a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    entity=\"vlad-aksiotis\",\n",
    "    project=\"myo_prost\",\n",
    "    log_model=True,\n",
    "    save_code=True,\n",
    "    notes=config.WANDB_NOTES,\n",
    "    dir=\"lightning_logs\",\n",
    "    tags=[\"data_v1\"],\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_angle_degree\",\n",
    "    save_top_k=5,\n",
    "    save_last=True,\n",
    "    filename=\"{epoch:02d}_{val_angle_degree:.3f}\",\n",
    "    verbose=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config.max_epochs,\n",
    "    #  accelerator= 'cpu', #'cuda',\n",
    "    accelerator=\"cpu\",\n",
    "    #  devices= 8 ,#config.device,\n",
    "    #  strategy=pl.strategies.Strategy(),\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        MovementsWandb(pred_fps=config.original_fps // config.down_sample_target),\n",
    "        checkpoint_callback,\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(model_pl, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
