{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987d8066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Установка seed для стандартного генератора случайных чисел Python\n",
    "random.seed(42)\n",
    "\n",
    "# Установка seed для NumPy (если используете его)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Установка seed для PyTorch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Если используете CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)  # Если у вас несколько GPU\n",
    "\n",
    "# Для обеспечения полной повторяемости (этот шаг замедляет выполнение на GPU)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3616af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "subj_path = \"..\\\\data\\\\Fedor\\\\Raw\\\\preproc_angles\\\\1\\\\\"\n",
    "fs = 500\n",
    "\n",
    "\n",
    "def corrcoef(x, y):\n",
    "    if np.std(x) == 0 or np.std(y) == 0:\n",
    "        return 0.0\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "\n",
    "def train_test_split(data, N_parts, num_of_part):\n",
    "    N_samples = len(data)\n",
    "\n",
    "    l_idx = int((N_samples * num_of_part) / N_parts)\n",
    "    h_idx = int((N_samples * (num_of_part + 1)) / N_parts)\n",
    "\n",
    "    data_train = np.concatenate([data[:l_idx, :], data[h_idx:,]], axis=0)\n",
    "    data_test = data[l_idx:h_idx, :]\n",
    "\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "N_files = 4\n",
    "data_list_train = list()\n",
    "data_list_test = list()\n",
    "label_list_train = list()\n",
    "label_list_test = list()\n",
    "\n",
    "N_parts = 10\n",
    "num_of_part = 9\n",
    "\n",
    "for i in range(N_files):\n",
    "\n",
    "    arr = np.load(subj_path + \"000\" + str(i) + \".npz\")\n",
    "\n",
    "    std_coef = arr[\"std_coef\"]\n",
    "    data = arr[\"data_myo\"]\n",
    "    label = arr[\"data_angles\"]\n",
    "\n",
    "    data_train, data_test = train_test_split(data, N_parts, num_of_part)\n",
    "    label_train, label_test = train_test_split(label, N_parts, num_of_part)\n",
    "\n",
    "    data_list_train.append(data_train)\n",
    "    data_list_test.append(data_test)\n",
    "\n",
    "    label_list_train.append(label_train)\n",
    "    label_list_test.append(label_test)\n",
    "\n",
    "\n",
    "data_train = np.concatenate(data_list_train, axis=0)\n",
    "data_test = np.concatenate(data_list_test, axis=0)\n",
    "\n",
    "\n",
    "label_train = np.concatenate(label_list_train, axis=0)\n",
    "label_test = np.concatenate(label_list_test, axis=0)\n",
    "\n",
    "\n",
    "def slicer(data, label, fs, windowlen=500, timestep=100):\n",
    "    data_len = len(data)\n",
    "    timestep_samples = int((timestep * fs) / 1000)\n",
    "    windowlen_samples = int((windowlen * fs) / 1000)\n",
    "    start_idc = np.arange(0, data_len - windowlen_samples, timestep_samples)[:, None]\n",
    "    window_idc = np.arange(0, windowlen_samples)[None, :]\n",
    "    slice_idc = start_idc + window_idc\n",
    "    slice_data = data[slice_idc].transpose(0, 2, 1)\n",
    "    slice_label = label[start_idc[:, 0] + windowlen_samples]\n",
    "    return slice_data, slice_label\n",
    "\n",
    "\n",
    "X_train, y_train = slicer(data_train, label_train, fs, windowlen=256, timestep=200)\n",
    "X_test, y_test = slicer(data_test, label_test, fs, windowlen=256, timestep=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5bf07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- PREPROCESSING FUNCTIONS ----------------\n",
    "\n",
    "\n",
    "def train_test_split(data, N_parts, num_of_part):\n",
    "    N_samples = len(data)\n",
    "    l_idx = int((N_samples * num_of_part) / N_parts)\n",
    "    h_idx = int((N_samples * (num_of_part + 1)) / N_parts)\n",
    "    data_train = np.concatenate([data[:l_idx], data[h_idx:]], axis=0)\n",
    "    data_test = data[l_idx:h_idx]\n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "def slicer(data, label, fs, windowlen=500, timestep=100):\n",
    "    data_len = len(data)\n",
    "    timestep_samples = int((timestep * fs) / 1000)\n",
    "    windowlen_samples = int((windowlen * fs) / 1000)\n",
    "    start_idc = np.arange(0, data_len - windowlen_samples, timestep_samples)[:, None]\n",
    "    window_idc = np.arange(0, windowlen_samples)[None, :]\n",
    "    slice_idc = start_idc + window_idc\n",
    "    slice_data = data[slice_idc].transpose(0, 2, 1)\n",
    "    slice_label = label[start_idc[:, 0] + windowlen_samples]\n",
    "    return slice_data, slice_label\n",
    "\n",
    "\n",
    "# ---------------- SEQUENCE PREPARATION ----------------\n",
    "\n",
    "\n",
    "def prepare_sequences(X, y, seq_len):\n",
    "    n_samples, feat_dim = X.shape\n",
    "    _, out_dim = y.shape\n",
    "    n_seq = n_samples - seq_len\n",
    "    X_seq = np.zeros((n_seq, seq_len, feat_dim), dtype=X.dtype)\n",
    "    y_seq = np.zeros((n_seq, seq_len, out_dim), dtype=y.dtype)\n",
    "    for i in range(n_seq):\n",
    "        X_seq[i] = X[i : i + seq_len]\n",
    "        y_seq[i] = y[i : i + seq_len]\n",
    "    return X_seq, y_seq\n",
    "\n",
    "\n",
    "def prepare_single_sequence(X, seq_len):\n",
    "    n_samples, feat_dim = X.shape\n",
    "    if n_samples < seq_len:\n",
    "        pad = np.zeros((seq_len - n_samples, feat_dim), dtype=X.dtype)\n",
    "        seq = np.vstack([pad, X])\n",
    "    else:\n",
    "        seq = X[-seq_len:]\n",
    "    return seq[np.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0b4bb78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([167, 10, 21])\n",
      "torch.Size([167, 10, 20])\n",
      "Epoch 10/100 Train Loss: 0.0055, Train Corr: 0.962, Val Loss: 0.0177, Val Corr: 0.931\n",
      "Epoch 20/100 Train Loss: 0.0036, Train Corr: 0.973, Val Loss: 0.0145, Val Corr: 0.941\n",
      "Epoch 30/100 Train Loss: 0.0027, Train Corr: 0.976, Val Loss: 0.0136, Val Corr: 0.946\n",
      "Epoch 40/100 Train Loss: 0.0023, Train Corr: 0.980, Val Loss: 0.0119, Val Corr: 0.950\n",
      "Epoch 50/100 Train Loss: 0.0019, Train Corr: 0.982, Val Loss: 0.0124, Val Corr: 0.948\n",
      "Epoch 60/100 Train Loss: 0.0017, Train Corr: 0.983, Val Loss: 0.0115, Val Corr: 0.951\n",
      "Epoch 70/100 Train Loss: 0.0017, Train Corr: 0.984, Val Loss: 0.0116, Val Corr: 0.951\n",
      "Epoch 80/100 Train Loss: 0.0014, Train Corr: 0.985, Val Loss: 0.0115, Val Corr: 0.951\n",
      "Epoch 90/100 Train Loss: 0.0014, Train Corr: 0.985, Val Loss: 0.0118, Val Corr: 0.951\n",
      "Epoch 100/100 Train Loss: 0.0014, Train Corr: 0.985, Val Loss: 0.0116, Val Corr: 0.951\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RiemannAutoRegressor(max_iter=100, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RiemannAutoRegressor</label><div class=\"sk-toggleable__content\"><pre>RiemannAutoRegressor(max_iter=100, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RiemannAutoRegressor(max_iter=100, verbose=True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TorchAutoRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim + output_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq, y_prev=None, teacher_forcing=False):\n",
    "        batch, seq_len, _ = x_seq.shape\n",
    "        device = x_seq.device\n",
    "        outputs = []\n",
    "        h, c = None, None\n",
    "        y_t = torch.zeros(batch, self.output_dim, device=device)\n",
    "        for t in range(seq_len):\n",
    "            x_t = x_seq[:, t]\n",
    "            inp = torch.cat([x_t, y_t], dim=-1).unsqueeze(1)\n",
    "            out_lstm, (h, c) = self.lstm(inp, (h, c) if h is not None else None)\n",
    "            y_pred = self.mlp(out_lstm[:, 0])\n",
    "            outputs.append(y_pred.unsqueeze(1))\n",
    "            y_t = y_prev[:, t] if (teacher_forcing and y_prev is not None) else y_pred\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "class RiemannAutoRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=\"oas\",\n",
    "        metric=\"riemann\",\n",
    "        hidden_dim=128,\n",
    "        seq_len=10,\n",
    "        num_layers=1,\n",
    "        alpha=1e-5,\n",
    "        lr=1e-3,\n",
    "        max_iter=1,\n",
    "        batch_size=32,\n",
    "        device=\"cpu\",\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.estimator = estimator\n",
    "        self.metric = metric\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "        self.cov_ = None\n",
    "        self.ts_ = None\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        # 1) Riemannian features\n",
    "        self.cov_ = Covariances(estimator=self.estimator)\n",
    "        X_cov = self.cov_.fit_transform(X)\n",
    "        self.ts_ = TangentSpace(metric=self.metric)\n",
    "        X_ts = self.ts_.fit_transform(X_cov)\n",
    "        # Validation transforms\n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_cov_val = self.cov_.transform(X_val)\n",
    "            X_ts_val = self.ts_.transform(X_cov_val)\n",
    "        # 2) Autoregressive sequences\n",
    "        X_seq, y_seq = prepare_sequences(X_ts, y, self.seq_len)\n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_seq_val, y_seq_val = prepare_sequences(X_ts_val, y_val, self.seq_len)\n",
    "        # to tensors\n",
    "        X_seq = torch.tensor(X_seq, dtype=torch.float32).to(self.device)\n",
    "        y_seq = torch.tensor(y_seq, dtype=torch.float32).to(self.device)\n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_seq_val = torch.tensor(X_seq_val, dtype=torch.float32).to(self.device)\n",
    "            y_seq_val = torch.tensor(y_seq_val, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        print(X_seq_val.shape)\n",
    "        print(y_seq_val.shape)\n",
    "        # dims\n",
    "        _, seq_len, feat_dim = X_seq.shape\n",
    "        _, _, out_dim = y_seq.shape\n",
    "        # 3) Model init\n",
    "        self.model_ = TorchAutoRegressor(\n",
    "            input_dim=feat_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=out_dim,\n",
    "            num_layers=self.num_layers,\n",
    "        ).to(self.device)\n",
    "        optimizer = optim.Adam(\n",
    "            self.model_.parameters(), lr=self.lr, weight_decay=self.alpha\n",
    "        )\n",
    "        criterion = nn.MSELoss()\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X_seq, y_seq),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        # Training loop\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            # train\n",
    "            self.model_.train()\n",
    "            total_loss = 0.0\n",
    "            for xb, yb in loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model_(xb, y_prev=yb, teacher_forcing=True)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            # metrics\n",
    "            if self.verbose and epoch % 10 == 0:\n",
    "                avg_train_loss = total_loss / len(loader)\n",
    "                # train corr\n",
    "                self.model_.eval()\n",
    "                with torch.no_grad():\n",
    "                    pred_train = self.model_(X_seq, teacher_forcing=False).cpu().numpy()\n",
    "                y_train_np = y_seq.cpu().numpy()\n",
    "                train_corrs = [\n",
    "                    corrcoef(pred_train[:, :, i].ravel(), y_train_np[:, :, i].ravel())\n",
    "                    for i in range(out_dim)\n",
    "                ]\n",
    "                train_mean_corr = np.nanmean(train_corrs)\n",
    "                msg = f\"Epoch {epoch}/{self.max_iter} Train Loss: {avg_train_loss:.4f}, Train Corr: {train_mean_corr:.3f}\"\n",
    "                # val metrics\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    with torch.no_grad():\n",
    "                        pred_val = (\n",
    "                            self.model_(X_seq_val, teacher_forcing=False).cpu().numpy()\n",
    "                        )\n",
    "                    y_val_np = y_seq_val.cpu().numpy()\n",
    "                    val_loss = criterion(\n",
    "                        torch.tensor(pred_val), torch.tensor(y_val_np)\n",
    "                    ).item()\n",
    "                    val_corrs = [\n",
    "                        corrcoef(pred_val[:, :, i].ravel(), y_val_np[:, :, i].ravel())\n",
    "                        for i in range(out_dim)\n",
    "                    ]\n",
    "                    val_mean_corr = np.nanmean(val_corrs)\n",
    "                    msg += f\", Val Loss: {val_loss:.4f}, Val Corr: {val_mean_corr:.3f}\"\n",
    "                print(msg)\n",
    "        return self\n",
    "\n",
    "    def validate(self, X, y):\n",
    "        X_cov = self.cov_.transform(X)\n",
    "        X_ts = self.ts_.transform(X_cov)\n",
    "        X_seq, y_seq = prepare_sequences(X_ts, y, self.seq_len)\n",
    "        X_seq = torch.tensor(X_seq, dtype=torch.float32).to(self.device)\n",
    "        # print(f\"X_seq shape: {X_seq.shape}\")\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            out_seq = self.model_(X_seq, teacher_forcing=False)\n",
    "        return out_seq.cpu().numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_cov = self.cov_.transform(X)\n",
    "        X_ts = self.ts_.transform(X_cov)\n",
    "        X_seq = prepare_single_sequence(X_ts, self.seq_len)\n",
    "        X_seq = torch.tensor(X_seq, dtype=torch.float32).to(self.device)\n",
    "        # print(f\"X_seq shape: {X_seq.shape}\")\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            out_seq = self.model_(X_seq, teacher_forcing=False)\n",
    "        return out_seq.cpu().numpy()\n",
    "\n",
    "\n",
    "model = RiemannAutoRegressor(\n",
    "    estimator=\"oas\",\n",
    "    metric=\"riemann\",\n",
    "    hidden_dim=128,\n",
    "    seq_len=10,\n",
    "    num_layers=1,\n",
    "    alpha=1e-5,\n",
    "    lr=1e-3,\n",
    "    max_iter=100,\n",
    "    batch_size=32,\n",
    "    device=\"cpu\",\n",
    "    verbose=True,\n",
    ")\n",
    "model.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7705ca72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred shape: (177, 20)\n",
      "Channel-wise correlations: [0.9707117539040602, 0.8687800360720954, 0.9725598389949479, 0.9735582471749029, 0.9575977468033179, 0.917247864256569, 0.9581943685852631, 0.9619693493628846, 0.9606472864103989, 0.9589279479103008, 0.9534198114240372, 0.9586765510305395, 0.9590291209717916, 0.9118572698984485, 0.9524557241819863, 0.9581176235394083, 0.9599937418651405, 0.9638377332921391, 0.958763275510492, 0.9646513412130475]\n",
      "Mean correlation: 0.9520498316200886\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1) вспомогательная функция корреляции\n",
    "def corrcoef_flat(x, y):\n",
    "    if np.std(x) == 0 or np.std(y) == 0:\n",
    "        return 0.0\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "\n",
    "# 2) делаем предсказания для каждого окна\n",
    "y_pred = []\n",
    "for i in range(len(X_test)):\n",
    "    # берем все окна до i-го включительно,\n",
    "    # чтобы модель имела предыдущие seq_len шагов\n",
    "    # (prepare_single_sequence внутри predict сама допадит нулями, если данных < seq_len).\n",
    "    input_seq = X_test[: i + 1]\n",
    "    # model.predict возвращает массив формы (1, seq_len, out_dim)\n",
    "    seq_pred = model.predict(input_seq)\n",
    "    # берем последний шаг предсказанной последовательности\n",
    "    y_t = seq_pred[0, -1, :]  # (out_dim,)\n",
    "    y_pred.append(y_t)\n",
    "\n",
    "y_pred = np.stack(y_pred, axis=0)  # (177, 20)\n",
    "\n",
    "# 3) считаем корреляцию по каждому из 20 каналов\n",
    "corrs = []\n",
    "for chan in range(y_test.shape[1]):\n",
    "    c = corrcoef_flat(y_pred[:, chan], y_test[:, chan])\n",
    "    corrs.append(c)\n",
    "\n",
    "mean_corr = np.nanmean(corrs)\n",
    "\n",
    "print(\"y_pred shape:\", y_pred.shape)  # (177, 20)\n",
    "print(\"Channel-wise correlations:\", corrs)  # список из 20 значений\n",
    "print(\"Mean correlation:\", mean_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "13ca9c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 20)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb388f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6ae63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b0cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f19b985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cov.shape: (1599, 6, 6)\n",
      "X_ts.shape: (1599, 21)\n",
      "y.shape: (1599, 20)\n",
      "X_seq.shape: (1589, 10, 21)\n",
      "y_seq.shape: (1589, 10, 20)\n",
      "Epoch 10/100 Loss: 0.0046\n",
      "Epoch 20/100 Loss: 0.0029\n",
      "Epoch 30/100 Loss: 0.0022\n",
      "Epoch 40/100 Loss: 0.0018\n",
      "Epoch 50/100 Loss: 0.0015\n",
      "Epoch 60/100 Loss: 0.0014\n",
      "Epoch 70/100 Loss: 0.0013\n",
      "Epoch 80/100 Loss: 0.0012\n",
      "Epoch 90/100 Loss: 0.0011\n",
      "Epoch 100/100 Loss: 0.0011\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RiemannAutoRegressor(verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RiemannAutoRegressor</label><div class=\"sk-toggleable__content\"><pre>RiemannAutoRegressor(verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RiemannAutoRegressor(verbose=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TorchAutoRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim + output_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq, y_prev=None, teacher_forcing=False):\n",
    "        batch, seq_len, _ = x_seq.shape\n",
    "        device = x_seq.device\n",
    "        outputs = []\n",
    "        h, c = None, None\n",
    "        y_t = torch.zeros(batch, self.output_dim, device=device)\n",
    "        for t in range(seq_len):\n",
    "            x_t = x_seq[:, t]\n",
    "            inp = torch.cat([x_t, y_t], dim=-1).unsqueeze(1)\n",
    "            out_lstm, (h, c) = self.lstm(inp, (h, c) if h is not None else None)\n",
    "            y_pred = self.mlp(out_lstm[:, 0])\n",
    "            outputs.append(y_pred.unsqueeze(1))\n",
    "            y_t = y_prev[:, t] if (teacher_forcing and y_prev is not None) else y_pred\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "class RiemannAutoRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=\"oas\",\n",
    "        metric=\"riemann\",\n",
    "        hidden_dim=128,\n",
    "        seq_len=10,\n",
    "        num_layers=1,\n",
    "        alpha=1e-5,\n",
    "        lr=1e-3,\n",
    "        max_iter=100,\n",
    "        batch_size=32,\n",
    "        device=\"cpu\",\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.estimator = estimator\n",
    "        self.metric = metric\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "        self.cov_ = None\n",
    "        self.ts_ = None\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 1) Riemannian features\n",
    "        self.cov_ = Covariances(estimator=self.estimator)\n",
    "        X_cov = self.cov_.fit_transform(X)\n",
    "        self.ts_ = TangentSpace(metric=self.metric)\n",
    "        X_ts = self.ts_.fit_transform(X_cov)\n",
    "\n",
    "        print(\"X_cov.shape:\", X_cov.shape)\n",
    "        print(\"X_ts.shape:\", X_ts.shape)\n",
    "        print(\"y.shape:\", y.shape)\n",
    "\n",
    "        # 2) Autoregressive sequences\n",
    "        X_seq, y_seq = prepare_sequences(X_ts, y, self.seq_len)\n",
    "\n",
    "        print(\"X_seq.shape:\", X_seq.shape)\n",
    "        print(\"y_seq.shape:\", y_seq.shape)\n",
    "\n",
    "        X_seq = torch.tensor(X_seq, dtype=torch.float32).to(self.device)\n",
    "        y_seq = torch.tensor(y_seq, dtype=torch.float32).to(self.device)\n",
    "        _, seq_len, feat_dim = X_seq.shape\n",
    "        _, _, out_dim = y_seq.shape\n",
    "        # 3) Model\n",
    "        self.model_ = TorchAutoRegressor(\n",
    "            input_dim=feat_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=out_dim,\n",
    "            num_layers=self.num_layers,\n",
    "        ).to(self.device)\n",
    "        optimizer = optim.Adam(\n",
    "            self.model_.parameters(), lr=self.lr, weight_decay=self.alpha\n",
    "        )\n",
    "        criterion = nn.MSELoss()\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X_seq, y_seq),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        # Training loop\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            self.model_.train()\n",
    "            total_loss = 0\n",
    "            for xb, yb in loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = self.model_(xb, y_prev=yb, teacher_forcing=True)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if self.verbose and epoch % 10 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}/{self.max_iter} Loss: {total_loss/len(loader):.4f}\"\n",
    "                )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 1) Riemannian features\n",
    "        X_cov = self.cov_.transform(X)\n",
    "        X_ts = self.ts_.transform(X_cov)\n",
    "\n",
    "        # 2) single sequence\n",
    "        X_seq = prepare_single_sequence(X_ts, self.seq_len)\n",
    "        X_seq = torch.tensor(X_seq, dtype=torch.float32).to(self.device)\n",
    "        # 3) autoregressive forward\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            out_seq = self.model_(X_seq, teacher_forcing=False)\n",
    "        return out_seq.cpu().numpy()\n",
    "\n",
    "\n",
    "model = RiemannAutoRegressor(\n",
    "    estimator=\"oas\",\n",
    "    metric=\"riemann\",\n",
    "    hidden_dim=128,\n",
    "    seq_len=10,\n",
    "    num_layers=1,\n",
    "    alpha=1e-5,\n",
    "    lr=1e-3,\n",
    "    max_iter=100,\n",
    "    batch_size=32,\n",
    "    device=\"cpu\",\n",
    "    verbose=True,\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25c27eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cov.shape: (1599, 6, 6)\n",
      "X_ts.shape: (1599, 21)\n",
      "y.shape: (1599, 20)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cov = Covariances(estimator=\"oas\")\n",
    "X_cov = cov.fit_transform(X_train)\n",
    "ts = TangentSpace(metric=\"riemann\")\n",
    "X_ts = ts.fit_transform(X_cov)\n",
    "\n",
    "print(\"X_cov.shape:\", X_cov.shape)\n",
    "print(\"X_ts.shape:\", X_ts.shape)\n",
    "print(\"y.shape:\", y_train.shape)\n",
    "\n",
    "# 2) Autoregressive sequences\n",
    "seq_len = 10\n",
    "X_seq, y_seq = prepare_sequences(X_ts, y_train, seq_len)\n",
    "\n",
    "X_seq = torch.tensor(X_seq, dtype=torch.float32).to(device)\n",
    "y_seq = torch.tensor(y_seq, dtype=torch.float32).to(device)\n",
    "_, seq_len, feat_dim = X_seq.shape\n",
    "_, _, out_dim = y_seq.shape\n",
    "hidden_dim = 128\n",
    "num_layers = 1\n",
    "\n",
    "model_ = TorchAutoRegressor(\n",
    "    input_dim=feat_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=out_dim,\n",
    "    num_layers=num_layers,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cb43e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, feat_dim = X_ts.shape\n",
    "_, out_dim = y_train.shape\n",
    "n_seq = n_samples - seq_len\n",
    "X_seq = np.zeros((n_seq, seq_len, feat_dim), dtype=X_ts.dtype)\n",
    "y_seq = np.zeros((n_seq, seq_len, out_dim), dtype=y_train.dtype)\n",
    "\n",
    "for i in range(n_seq):\n",
    "    X_seq[i] = X_ts[i : i + seq_len]\n",
    "    y_seq[i] = y_train[i : i + seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6f5869e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64358705, -0.23727681, -0.04386122,  0.00412455, -0.0709638 ,\n",
       "       -0.26395055,  2.38228933, -0.337386  , -0.14176192, -0.2605633 ,\n",
       "       -0.03096146,  1.61970657, -0.1942402 , -0.30699109, -0.01986674,\n",
       "        2.63622328,  0.18088524, -0.49958034,  3.38308603,  0.21076641,\n",
       "        2.45148874])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6a1dde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.64358705e+00, -2.37276809e-01, -4.38612234e-02,\n",
       "         4.12454531e-03, -7.09638004e-02, -2.63950547e-01,\n",
       "         2.38228933e+00, -3.37385996e-01, -1.41761915e-01,\n",
       "        -2.60563304e-01, -3.09614618e-02,  1.61970657e+00,\n",
       "        -1.94240196e-01, -3.06991090e-01, -1.98667419e-02,\n",
       "         2.63622328e+00,  1.80885241e-01, -4.99580341e-01,\n",
       "         3.38308603e+00,  2.10766413e-01,  2.45148874e+00],\n",
       "       [ 9.65203125e-01, -1.36086982e-01, -3.36915621e-02,\n",
       "        -6.84868190e-02,  1.47824083e-02, -1.36514497e-01,\n",
       "         1.54823310e+00, -4.62258424e-01, -5.16654675e-01,\n",
       "        -3.44370574e-01,  5.78856784e-02,  9.70422307e-01,\n",
       "        -2.99419309e-01, -5.16909240e-01,  1.39671774e-01,\n",
       "         2.24391003e+00, -1.06928707e-01, -3.17612965e-01,\n",
       "         2.43673472e+00,  2.43861437e-01,  1.60795875e+00],\n",
       "       [ 7.43040159e-01,  1.27627244e-01,  3.18447851e-01,\n",
       "        -1.98260873e-01, -3.65146489e-01, -8.22752208e-02,\n",
       "         5.85480338e-01,  3.08336241e-02, -1.50135962e-01,\n",
       "         3.31871374e-02, -1.28455929e-01,  3.46623118e+00,\n",
       "        -5.60224141e-02, -3.32732388e-02,  2.51408130e-02,\n",
       "         1.60521533e+00, -7.26145949e-01, -6.02053224e-01,\n",
       "         2.73014194e+00, -1.28158058e-01,  1.15157997e+00],\n",
       "       [ 9.90552143e-01,  5.58312333e-02,  3.48736991e-01,\n",
       "         1.58186854e-02, -2.64640130e-01, -9.03671980e-02,\n",
       "         1.40201305e+00,  2.97361879e-01,  5.81545561e-02,\n",
       "         1.13458668e-01, -5.43694209e-02,  4.11385713e+00,\n",
       "         6.11050249e-02, -9.27218341e-02, -2.15854575e-02,\n",
       "         2.29487196e+00, -3.91845346e-01, -2.56134504e-01,\n",
       "         3.11605305e+00, -2.50839709e-01,  1.03446270e+00],\n",
       "       [ 3.68010626e-02,  7.11997446e-02,  2.39318000e-01,\n",
       "        -1.64763082e-01, -1.95192946e-01,  3.06308190e-01,\n",
       "         5.48326003e-01,  3.86823446e-01,  2.81850331e-01,\n",
       "         6.99052452e-02, -1.74980786e-01,  3.15810747e+00,\n",
       "        -7.34448378e-02, -2.19801403e-01, -1.95422177e-01,\n",
       "         1.91739766e+00, -4.49480709e-01, -4.30298501e-01,\n",
       "         1.66137369e+00, -2.49649587e-01,  1.87581677e-01],\n",
       "       [-3.89208310e-01,  2.06847766e-01,  1.20819149e-01,\n",
       "        -3.87339783e-01, -1.61762956e-01,  3.33413788e-01,\n",
       "         5.82268824e-02,  2.28034140e-01,  2.13707824e-01,\n",
       "        -1.94329924e-01, -1.10133956e-01,  8.49006927e-01,\n",
       "         8.39308667e-01, -1.49893706e-01, -6.40484427e-02,\n",
       "         4.51965679e-01, -3.40442913e-01, -2.69839444e-01,\n",
       "         1.54960362e-01,  2.47648387e-01, -3.91945634e-01],\n",
       "       [ 2.06496567e-01, -1.12931748e-01,  1.65838878e-01,\n",
       "         9.37673431e-02, -1.72729961e-01, -4.73296703e-01,\n",
       "         3.26816436e-01,  1.44644201e-02, -7.62843469e-02,\n",
       "        -1.29732112e-01, -1.71364513e-01,  4.97714516e-01,\n",
       "         4.51282499e-01, -1.75453226e-01, -2.40498175e-01,\n",
       "         5.03765151e-01, -4.33893647e-02, -4.09673503e-01,\n",
       "         1.04910991e+00,  5.95189405e-01,  1.80235525e+00],\n",
       "       [ 1.47434449e+00, -1.51696564e-01,  2.87257040e-01,\n",
       "         1.81177794e-01, -1.97137065e-01, -5.92560712e-01,\n",
       "         2.04177153e+00, -2.51103730e-01,  2.42405783e-02,\n",
       "        -3.36939729e-01, -1.36477929e-01,  1.07323060e+00,\n",
       "        -1.52632027e-01, -2.21822237e-01, -2.28462440e-01,\n",
       "         2.00027697e+00, -4.53174667e-01, -6.82256303e-01,\n",
       "         2.45818434e+00,  3.44900822e-01,  2.92642311e+00],\n",
       "       [ 1.67033180e+00, -1.39178169e-01,  2.51427384e-02,\n",
       "         3.45194838e-02, -3.20476080e-01, -2.24232120e-01,\n",
       "         2.53813070e+00, -3.65312343e-01, -3.05335852e-01,\n",
       "        -3.39604656e-02, -3.37071562e-02,  1.31731256e+00,\n",
       "        -1.37546175e-01, -2.28283374e-01,  1.02260463e-01,\n",
       "         2.59907832e+00, -2.88735224e-01, -3.92748878e-01,\n",
       "         2.89798417e+00, -3.32882799e-02,  2.53545355e+00],\n",
       "       [ 1.60572004e+00, -2.13475457e-01,  4.39015652e-02,\n",
       "         5.32126832e-02, -2.73531870e-01,  2.06095928e-02,\n",
       "         2.25531597e+00, -4.07823749e-01, -3.24415642e-01,\n",
       "        -1.60480182e-01,  4.04400685e-03,  1.41827690e+00,\n",
       "        -1.09477759e-01, -7.30995347e-02,  1.88446697e-01,\n",
       "         2.23935942e+00, -4.46220940e-01, -1.10309009e-01,\n",
       "         2.82538792e+00, -1.38222532e-01,  2.06310391e+00]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13d1ed10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1f24c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 21)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "X_ts[i : i + seq_len].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd9539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(X, y, seq_len):\n",
    "    \"\"\"\n",
    "    Преобразует плоские данные X и y в перекрывающиеся автопрегрессионные последовательности.\n",
    "    X: numpy array, shape (n_samples, feat_dim)\n",
    "    y: numpy array, shape (n_samples, output_dim)\n",
    "    seq_len: int, длина последовательности\n",
    "\n",
    "    Возвращает:\n",
    "    X_seq: numpy array, shape (n_seq, seq_len, feat_dim)\n",
    "    y_seq: numpy array, shape (n_seq, seq_len, output_dim)\n",
    "    где n_seq = n_samples - seq_len\n",
    "    \"\"\"\n",
    "    n_samples, feat_dim = X.shape\n",
    "    _, out_dim = y.shape\n",
    "    n_seq = n_samples - seq_len\n",
    "    X_seq = np.zeros((n_seq, seq_len, feat_dim), dtype=X.dtype)\n",
    "    y_seq = np.zeros((n_seq, seq_len, out_dim), dtype=y.dtype)\n",
    "    for i in range(n_seq):\n",
    "        X_seq[i] = X[i : i + seq_len]\n",
    "        y_seq[i] = y[i : i + seq_len]\n",
    "    return X_seq, y_seq\n",
    "\n",
    "\n",
    "def prepare_single_sequence(X, seq_len):\n",
    "    \"\"\"\n",
    "    Готовит единичную последовательность из последних seq_len сэмплов X.\n",
    "    X: numpy array, shape (n_samples, feat_dim)\n",
    "    seq_len: int\n",
    "\n",
    "    Возвращает numpy array с формой (1, seq_len, feat_dim)\n",
    "    \"\"\"\n",
    "    n_samples, feat_dim = X.shape\n",
    "    if n_samples < seq_len:\n",
    "        # дополняем нулями вперед\n",
    "        pad = np.zeros((seq_len - n_samples, feat_dim), dtype=X.dtype)\n",
    "        seq = np.vstack([pad, X])\n",
    "    else:\n",
    "        seq = X[-seq_len:]\n",
    "    return seq[np.newaxis, ...]\n",
    "\n",
    "\n",
    "class TorchAutoRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        # LSTM: вход = [признаки + предыдущие предсказания]\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim + output_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        # Голова MLP для предсказания углов\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_seq, y_prev=None, teacher_forcing=False):\n",
    "        # x_seq: (batch, seq_len, feat_dim)\n",
    "        # y_prev: (batch, seq_len, output_dim)\n",
    "        batch, seq_len, _ = x_seq.shape\n",
    "        device = x_seq.device\n",
    "        outputs = []\n",
    "        h, c = None, None\n",
    "        # начальное предыдущее предсказание = нули\n",
    "        y_t = torch.zeros(batch, self.output_dim, device=device)\n",
    "        for t in range(seq_len):\n",
    "            x_t = x_seq[:, t]\n",
    "            inp = torch.cat([x_t, y_t], dim=-1).unsqueeze(1)  # (batch,1,feat+out)\n",
    "            out_lstm, (h, c) = self.lstm(inp, (h, c) if h is not None else None)\n",
    "            y_pred = self.mlp(out_lstm[:, 0])  # (batch, output_dim)\n",
    "            outputs.append(y_pred.unsqueeze(1))\n",
    "            if teacher_forcing and (y_prev is not None):\n",
    "                y_t = y_prev[:, t]\n",
    "            else:\n",
    "                y_t = y_pred\n",
    "        return torch.cat(outputs, dim=1)  # (batch, seq_len, output_dim)\n",
    "\n",
    "\n",
    "class RiemannAutoRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=\"oas\",\n",
    "        metric=\"riemann\",\n",
    "        hidden_dim=128,\n",
    "        seq_len=10,\n",
    "        num_layers=1,\n",
    "        alpha=1e-5,\n",
    "        lr=1e-3,\n",
    "        max_iter=100,\n",
    "        batch_size=32,\n",
    "        device=\"cpu\",\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.estimator = estimator\n",
    "        self.metric = metric\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "        self.cov_ = None\n",
    "        self.ts_ = None\n",
    "        self.model_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # X: (n_samples, n_channels, n_times)\n",
    "        # y: (n_samples, output_dim)\n",
    "        # Шаг 1: ковариации + тангенциальное пространство\n",
    "        self.cov_ = Covariances(estimator=self.estimator)\n",
    "        X_cov = self.cov_.fit_transform(X)\n",
    "        self.ts_ = TangentSpace(metric=self.metric)\n",
    "        X_ts = self.ts_.fit_transform(X_cov)\n",
    "\n",
    "        # Шаг 2: подготовка автопрогнозных последовательностей\n",
    "        X_seq, y_seq = prepare_sequences(X_ts, y, self.seq_len)\n",
    "\n",
    "        # Конвертация в тензоры\n",
    "        X_seq = torch.tensor(X_seq, dtype=torch.float32).to(self.device)\n",
    "        y_seq = torch.tensor(y_seq, dtype=torch.float32).to(self.device)\n",
    "        batch, seq_len, feat_dim = X_seq.shape\n",
    "        _, _, out_dim = y_seq.shape\n",
    "\n",
    "        # Инициализация модели\n",
    "        self.model_ = TorchAutoRegressor(\n",
    "            input_dim=feat_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=out_dim,\n",
    "            num_layers=self.num_layers,\n",
    "        ).to(self.device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(\n",
    "            self.model_.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.alpha,\n",
    "        )\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(X_seq, y_seq),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        # Обучение\n",
    "        for epoch in range(1, self.max_iter + 1):\n",
    "            self.model_.train()\n",
    "            total_loss = 0.0\n",
    "            for x_batch, y_batch in loader:\n",
    "                optimizer.zero_grad()\n",
    "                out_seq = self.model_(x_batch, y_prev=y_batch, teacher_forcing=True)\n",
    "                loss = criterion(out_seq, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if self.verbose and (epoch % 10 == 0):\n",
    "                avg_loss = total_loss / len(loader)\n",
    "                print(f\"Epoch {epoch}/{self.max_iter}, Loss: {avg_loss:.4f}\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # X: (n_samples, n_channels, n_times)\n",
    "        # Шаг 1: ковариации + тангенциальное пространство\n",
    "        X_cov = self.cov_.transform(X)\n",
    "        X_ts = self.ts_.transform(X_cov)\n",
    "        # Шаг 2: подготовка единичной последовательности\n",
    "        X_seq = prepare_single_sequence(X_ts, self.seq_len)\n",
    "        X_seq = torch.tensor(X_seq, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            out_seq = self.model_(X_seq, teacher_forcing=False)\n",
    "        # Выход: последовательность прогноза (1, seq_len, out_dim)\n",
    "        return out_seq.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67fc56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchAutoRegressor(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden_size, output_dim, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feat_dim + output_dim,  # прибавляем прошлое y\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, Z_seq, y_prev=None, teacher_forcing_ratio=0.5):\n",
    "        # Z_seq: (batch, T, feat_dim)\n",
    "        batch, T, _ = Z_seq.shape\n",
    "        device = Z_seq.device\n",
    "        # инициализация предыдущего y нулями\n",
    "        if y_prev is None:\n",
    "            y_prev = torch.zeros(batch, self.linear.out_features, device=device)\n",
    "        h, c = None, None\n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            zt = Z_seq[:, t]  # (batch, feat_dim)\n",
    "            inp = torch.cat([zt, y_prev], dim=1)  # (batch, feat_dim+output_dim)\n",
    "            out_lstm, (h, c) = self.lstm(inp.unsqueeze(1), (h, c))  # (b,1,hidden)\n",
    "            y_t = self.linear(out_lstm[:, 0, :])  # (b, output_dim)\n",
    "            outputs.append(y_t.unsqueeze(1))\n",
    "            # teacher forcing\n",
    "            if self.training and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                # вместо собственного предсказания берём «правильное» y_true,\n",
    "                # нужно передать y_true_seq в forward\n",
    "                y_prev = teacher_y[:, t, :]\n",
    "            else:\n",
    "                y_prev = y_t\n",
    "        return torch.cat(outputs, dim=1)  # (batch, T, output_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
